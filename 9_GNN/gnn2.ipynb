{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>element</th>\n",
       "      <th>atom_type</th>\n",
       "      <th>charge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>h</td>\n",
       "      <td>3</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  drug_id element  atom_type  charge\n",
       "0   0        0       c         22   -0.11\n",
       "1   1        0       c         22   -0.11\n",
       "2   2        0       c         22   -0.11\n",
       "3   3        0       c         22   -0.11\n",
       "4   4        0       h          3    0.15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom = pd.read_csv(\n",
    "    \"dataset/Mutagenesis-42/atoms.csv\",\n",
    "    delimiter=\";\",\n",
    ")\n",
    "bond = pd.read_csv(\"dataset/Mutagenesis-42/bonds.csv\", delimiter=\";\")\n",
    "molecule = pd.read_csv(\"dataset/Mutagenesis-42/drugs.csv\", delimiter=\";\")\n",
    "atom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom[\"atom_type\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ind1</th>\n",
       "      <th>inda</th>\n",
       "      <th>act</th>\n",
       "      <th>logp</th>\n",
       "      <th>lumo</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>2.29</td>\n",
       "      <td>-3.025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4.35</td>\n",
       "      <td>-2.138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-1.429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-1.478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ind1  inda   act  logp   lumo  active\n",
       "0   0   0.0   0.0 -0.70  2.29 -3.025       0\n",
       "1   1   0.0   0.0  0.57  2.13 -0.798       1\n",
       "2   2   1.0   0.0  0.77  4.35 -2.138       1\n",
       "3   3   1.0   0.0 -0.22  5.41 -1.429       0\n",
       "4   4   1.0   0.0 -0.22  5.41 -1.478       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule_ids = molecule[\"id\"].unique().tolist()\n",
    "molecule_ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_dict = bond.values.tolist()\n",
    "edges_dict = {(s[2], s[3]): s[4] for s in edges_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 11): 7,\n",
       " (11, 19): 7,\n",
       " (19, 20): 7,\n",
       " (20, 21): 7,\n",
       " (21, 22): 7,\n",
       " (22, 0): 7,\n",
       " (11, 23): 1,\n",
       " (22, 24): 1,\n",
       " (19, 25): 7,\n",
       " (25, 1): 7,\n",
       " (1, 2): 7,\n",
       " (2, 3): 7,\n",
       " (3, 20): 7,\n",
       " (25, 4): 1,\n",
       " (2, 5): 1,\n",
       " (3, 6): 1,\n",
       " (1, 7): 1,\n",
       " (21, 8): 1,\n",
       " (0, 9): 1,\n",
       " (8, 10): 2,\n",
       " (8, 12): 2,\n",
       " (6, 13): 2,\n",
       " (6, 14): 2,\n",
       " (7, 15): 2,\n",
       " (7, 16): 2,\n",
       " (9, 17): 2,\n",
       " (9, 18): 2,\n",
       " (26, 36): 7,\n",
       " (36, 37): 7,\n",
       " (37, 38): 7,\n",
       " (38, 39): 7,\n",
       " (39, 40): 7,\n",
       " (40, 26): 7,\n",
       " (37, 41): 7,\n",
       " (41, 42): 7,\n",
       " (38, 43): 7,\n",
       " (43, 42): 7,\n",
       " (40, 27): 1,\n",
       " (27, 28): 2,\n",
       " (27, 29): 2,\n",
       " (41, 30): 1,\n",
       " (26, 31): 1,\n",
       " (36, 32): 1,\n",
       " (39, 33): 1,\n",
       " (42, 34): 1,\n",
       " (43, 35): 1,\n",
       " (44, 55): 7,\n",
       " (55, 66): 7,\n",
       " (66, 68): 7,\n",
       " (68, 69): 7,\n",
       " (69, 70): 7,\n",
       " (70, 44): 7,\n",
       " (68, 71): 7,\n",
       " (71, 72): 7,\n",
       " (72, 73): 7,\n",
       " (73, 45): 7,\n",
       " (45, 69): 7,\n",
       " (71, 46): 7,\n",
       " (46, 47): 7,\n",
       " (47, 48): 7,\n",
       " (48, 49): 7,\n",
       " (49, 72): 7,\n",
       " (66, 50): 7,\n",
       " (50, 51): 7,\n",
       " (51, 46): 7,\n",
       " (47, 52): 7,\n",
       " (51, 53): 7,\n",
       " (53, 52): 7,\n",
       " (44, 54): 1,\n",
       " (55, 56): 1,\n",
       " (70, 57): 1,\n",
       " (73, 58): 1,\n",
       " (45, 59): 1,\n",
       " (48, 60): 1,\n",
       " (49, 61): 1,\n",
       " (50, 62): 1,\n",
       " (52, 63): 1,\n",
       " (53, 64): 1,\n",
       " (64, 65): 2,\n",
       " (64, 67): 2,\n",
       " (74, 85): 7,\n",
       " (85, 96): 7,\n",
       " (96, 100): 7,\n",
       " (100, 101): 7,\n",
       " (101, 102): 7,\n",
       " (102, 74): 7,\n",
       " (74, 103): 1,\n",
       " (85, 104): 1,\n",
       " (101, 105): 1,\n",
       " (102, 75): 1,\n",
       " (96, 76): 7,\n",
       " (76, 77): 7,\n",
       " (77, 78): 7,\n",
       " (78, 79): 7,\n",
       " (79, 100): 7,\n",
       " (76, 80): 1,\n",
       " (78, 81): 7,\n",
       " (81, 82): 7,\n",
       " (82, 83): 7,\n",
       " (83, 84): 7,\n",
       " (84, 79): 7,\n",
       " (83, 86): 1,\n",
       " (84, 87): 1,\n",
       " (81, 88): 7,\n",
       " (88, 89): 7,\n",
       " (89, 90): 7,\n",
       " (90, 91): 7,\n",
       " (91, 82): 7,\n",
       " (88, 92): 1,\n",
       " (89, 93): 1,\n",
       " (90, 94): 1,\n",
       " (91, 95): 1,\n",
       " (77, 97): 1,\n",
       " (97, 98): 2,\n",
       " (97, 99): 2,\n",
       " (106, 117): 7,\n",
       " (117, 128): 7,\n",
       " (128, 132): 7,\n",
       " (132, 133): 7,\n",
       " (133, 134): 7,\n",
       " (134, 106): 7,\n",
       " (106, 135): 1,\n",
       " (117, 136): 1,\n",
       " (133, 137): 1,\n",
       " (134, 107): 1,\n",
       " (128, 108): 7,\n",
       " (108, 109): 7,\n",
       " (109, 110): 7,\n",
       " (110, 111): 7,\n",
       " (111, 132): 7,\n",
       " (108, 112): 1,\n",
       " (110, 113): 7,\n",
       " (113, 114): 7,\n",
       " (114, 115): 7,\n",
       " (115, 116): 7,\n",
       " (116, 111): 7,\n",
       " (115, 118): 1,\n",
       " (116, 119): 1,\n",
       " (113, 120): 7,\n",
       " (120, 121): 7,\n",
       " (121, 122): 7,\n",
       " (122, 123): 7,\n",
       " (123, 114): 7,\n",
       " (120, 124): 1,\n",
       " (121, 125): 1,\n",
       " (123, 126): 1,\n",
       " (109, 127): 1,\n",
       " (122, 129): 1,\n",
       " (129, 130): 2,\n",
       " (129, 131): 2,\n",
       " (138, 148): 7,\n",
       " (148, 149): 7,\n",
       " (149, 150): 7,\n",
       " (150, 151): 7,\n",
       " (151, 152): 7,\n",
       " (152, 138): 7,\n",
       " (138, 153): 1,\n",
       " (149, 154): 1,\n",
       " (152, 155): 1,\n",
       " (151, 139): 1,\n",
       " (148, 140): 1,\n",
       " (139, 141): 1,\n",
       " (139, 142): 1,\n",
       " (140, 143): 2,\n",
       " (140, 144): 2,\n",
       " (150, 145): 1,\n",
       " (145, 146): 1,\n",
       " (145, 147): 1,\n",
       " (156, 162): 7,\n",
       " (162, 163): 7,\n",
       " (163, 164): 7,\n",
       " (164, 165): 7,\n",
       " (165, 166): 7,\n",
       " (166, 156): 7,\n",
       " (156, 167): 1,\n",
       " (166, 168): 1,\n",
       " (165, 169): 1,\n",
       " (164, 157): 1,\n",
       " (163, 158): 1,\n",
       " (162, 159): 1,\n",
       " (159, 160): 2,\n",
       " (159, 161): 2,\n",
       " (170, 181): 7,\n",
       " (181, 187): 7,\n",
       " (187, 188): 7,\n",
       " (188, 189): 7,\n",
       " (189, 190): 7,\n",
       " (190, 170): 7,\n",
       " (170, 191): 1,\n",
       " (181, 192): 1,\n",
       " (189, 193): 1,\n",
       " (190, 171): 1,\n",
       " (172, 173): 7,\n",
       " (173, 174): 7,\n",
       " (174, 175): 7,\n",
       " (175, 176): 7,\n",
       " (176, 177): 7,\n",
       " (177, 172): 7,\n",
       " (173, 178): 1,\n",
       " (174, 179): 1,\n",
       " (188, 177): 1,\n",
       " (187, 180): 1,\n",
       " (180, 172): 1,\n",
       " (180, 182): 1,\n",
       " (175, 183): 1,\n",
       " (176, 184): 1,\n",
       " (184, 185): 2,\n",
       " (184, 186): 2,\n",
       " (194, 205): 7,\n",
       " (205, 207): 7,\n",
       " (207, 208): 7,\n",
       " (208, 209): 7,\n",
       " (209, 210): 7,\n",
       " (210, 194): 7,\n",
       " (207, 211): 7,\n",
       " (211, 212): 7,\n",
       " (208, 213): 7,\n",
       " (213, 212): 7,\n",
       " (211, 195): 1,\n",
       " (212, 196): 1,\n",
       " (196, 197): 2,\n",
       " (196, 198): 2,\n",
       " (194, 199): 1,\n",
       " (205, 200): 1,\n",
       " (209, 201): 1,\n",
       " (210, 202): 1,\n",
       " (195, 203): 1,\n",
       " (195, 204): 1,\n",
       " (195, 206): 1,\n",
       " (406, 417): 7,\n",
       " (417, 428): 7,\n",
       " (428, 434): 7,\n",
       " (434, 435): 7,\n",
       " (435, 436): 7,\n",
       " (436, 406): 7,\n",
       " (406, 437): 1,\n",
       " (417, 438): 1,\n",
       " (435, 439): 1,\n",
       " (436, 407): 1,\n",
       " (428, 408): 7,\n",
       " (408, 409): 7,\n",
       " (409, 410): 7,\n",
       " (410, 411): 7,\n",
       " (411, 434): 7,\n",
       " (409, 412): 7,\n",
       " (412, 413): 7,\n",
       " (413, 414): 7,\n",
       " (414, 415): 7,\n",
       " (415, 410): 7,\n",
       " (412, 416): 1,\n",
       " (413, 418): 1,\n",
       " (415, 419): 7,\n",
       " (419, 420): 7,\n",
       " (420, 421): 7,\n",
       " (421, 411): 7,\n",
       " (420, 422): 1,\n",
       " (421, 423): 1,\n",
       " (414, 424): 7,\n",
       " (424, 425): 7,\n",
       " (425, 426): 7,\n",
       " (426, 419): 7,\n",
       " (424, 427): 1,\n",
       " (425, 429): 1,\n",
       " (426, 430): 1,\n",
       " (408, 431): 1,\n",
       " (431, 432): 2,\n",
       " (431, 433): 2,\n",
       " (214, 224): 7,\n",
       " (224, 225): 7,\n",
       " (225, 226): 7,\n",
       " (226, 227): 7,\n",
       " (227, 228): 7,\n",
       " (228, 214): 7,\n",
       " (225, 229): 7,\n",
       " (229, 230): 7,\n",
       " (226, 231): 7,\n",
       " (231, 230): 7,\n",
       " (224, 215): 1,\n",
       " (215, 216): 2,\n",
       " (215, 217): 2,\n",
       " (214, 218): 1,\n",
       " (227, 219): 1,\n",
       " (228, 220): 1,\n",
       " (230, 221): 1,\n",
       " (231, 222): 1,\n",
       " (229, 223): 1,\n",
       " (232, 238): 7,\n",
       " (238, 239): 7,\n",
       " (239, 240): 7,\n",
       " (240, 241): 7,\n",
       " (241, 242): 7,\n",
       " (242, 232): 7,\n",
       " (232, 243): 1,\n",
       " (238, 244): 1,\n",
       " (242, 245): 1,\n",
       " (241, 233): 1,\n",
       " (233, 234): 2,\n",
       " (233, 235): 2,\n",
       " (240, 236): 1,\n",
       " (239, 237): 1,\n",
       " (246, 254): 7,\n",
       " (254, 255): 7,\n",
       " (255, 256): 7,\n",
       " (256, 257): 7,\n",
       " (257, 258): 7,\n",
       " (258, 246): 7,\n",
       " (246, 259): 1,\n",
       " (254, 260): 1,\n",
       " (258, 261): 1,\n",
       " (257, 247): 1,\n",
       " (247, 248): 2,\n",
       " (247, 249): 2,\n",
       " (255, 250): 1,\n",
       " (256, 251): 1,\n",
       " (251, 252): 2,\n",
       " (251, 253): 2,\n",
       " (262, 268): 7,\n",
       " (268, 269): 7,\n",
       " (269, 270): 7,\n",
       " (270, 271): 7,\n",
       " (271, 272): 7,\n",
       " (272, 262): 7,\n",
       " (262, 273): 1,\n",
       " (268, 274): 1,\n",
       " (272, 275): 1,\n",
       " (271, 263): 1,\n",
       " (263, 264): 2,\n",
       " (263, 265): 2,\n",
       " (270, 266): 1,\n",
       " (267, 269): 1,\n",
       " (276, 282): 7,\n",
       " (282, 283): 7,\n",
       " (283, 284): 7,\n",
       " (284, 285): 7,\n",
       " (285, 286): 7,\n",
       " (286, 276): 7,\n",
       " (276, 287): 1,\n",
       " (286, 288): 1,\n",
       " (285, 289): 1,\n",
       " (289, 277): 2,\n",
       " (289, 278): 2,\n",
       " (284, 279): 1,\n",
       " (282, 280): 1,\n",
       " (283, 281): 1,\n",
       " (290, 301): 7,\n",
       " (301, 312): 7,\n",
       " (312, 316): 7,\n",
       " (316, 317): 7,\n",
       " (317, 318): 7,\n",
       " (318, 290): 7,\n",
       " (290, 319): 1,\n",
       " (301, 320): 1,\n",
       " (317, 321): 1,\n",
       " (318, 291): 1,\n",
       " (312, 292): 7,\n",
       " (292, 293): 7,\n",
       " (293, 294): 7,\n",
       " (294, 295): 7,\n",
       " (295, 316): 7,\n",
       " (295, 296): 1,\n",
       " (293, 297): 7,\n",
       " (297, 298): 7,\n",
       " (298, 299): 7,\n",
       " (299, 300): 7,\n",
       " (300, 294): 7,\n",
       " (297, 302): 1,\n",
       " (298, 303): 1,\n",
       " (299, 304): 7,\n",
       " (304, 305): 7,\n",
       " (305, 306): 7,\n",
       " (306, 307): 7,\n",
       " (307, 300): 7,\n",
       " (304, 308): 1,\n",
       " (305, 309): 1,\n",
       " (306, 310): 1,\n",
       " (307, 311): 1,\n",
       " (292, 313): 1,\n",
       " (313, 314): 2,\n",
       " (313, 315): 2,\n",
       " (322, 328): 7,\n",
       " (328, 329): 7,\n",
       " (329, 330): 7,\n",
       " (330, 331): 7,\n",
       " (331, 332): 7,\n",
       " (332, 322): 7,\n",
       " (322, 333): 1,\n",
       " (332, 334): 1,\n",
       " (331, 335): 1,\n",
       " (335, 323): 2,\n",
       " (335, 324): 2,\n",
       " (328, 325): 1,\n",
       " (329, 326): 1,\n",
       " (330, 327): 1,\n",
       " (336, 347): 7,\n",
       " (347, 355): 7,\n",
       " (355, 356): 7,\n",
       " (356, 357): 7,\n",
       " (357, 358): 7,\n",
       " (358, 336): 7,\n",
       " (336, 359): 1,\n",
       " (347, 360): 1,\n",
       " (357, 361): 1,\n",
       " (358, 337): 1,\n",
       " (355, 338): 7,\n",
       " (338, 339): 7,\n",
       " (339, 340): 7,\n",
       " (340, 341): 7,\n",
       " (341, 356): 7,\n",
       " (338, 342): 1,\n",
       " (339, 343): 7,\n",
       " (343, 344): 7,\n",
       " (344, 345): 7,\n",
       " (345, 346): 7,\n",
       " (346, 340): 7,\n",
       " (343, 348): 1,\n",
       " (344, 349): 1,\n",
       " (345, 350): 1,\n",
       " (346, 351): 1,\n",
       " (341, 352): 1,\n",
       " (352, 353): 2,\n",
       " (352, 354): 2,\n",
       " (362, 368): 7,\n",
       " (368, 369): 7,\n",
       " (369, 370): 7,\n",
       " (370, 371): 7,\n",
       " (371, 372): 7,\n",
       " (372, 362): 7,\n",
       " (362, 373): 1,\n",
       " (372, 374): 1,\n",
       " (371, 375): 1,\n",
       " (375, 363): 2,\n",
       " (375, 364): 2,\n",
       " (368, 365): 1,\n",
       " (370, 366): 1,\n",
       " (369, 367): 1,\n",
       " (376, 387): 7,\n",
       " (387, 398): 7,\n",
       " (398, 400): 7,\n",
       " (400, 401): 7,\n",
       " (401, 402): 7,\n",
       " (402, 376): 7,\n",
       " (398, 403): 7,\n",
       " (403, 404): 7,\n",
       " (404, 405): 7,\n",
       " (405, 377): 7,\n",
       " (377, 400): 7,\n",
       " (404, 378): 1,\n",
       " (378, 379): 2,\n",
       " (379, 380): 1,\n",
       " (380, 381): 1,\n",
       " (381, 405): 1,\n",
       " (377, 382): 1,\n",
       " (382, 383): 2,\n",
       " (382, 384): 2,\n",
       " (376, 385): 1,\n",
       " (387, 386): 1,\n",
       " (401, 388): 1,\n",
       " (402, 389): 1,\n",
       " (403, 390): 1,\n",
       " (378, 391): 1,\n",
       " (379, 392): 1,\n",
       " (381, 393): 5,\n",
       " (381, 394): 1,\n",
       " (380, 395): 4,\n",
       " (380, 396): 1,\n",
       " (393, 397): 1,\n",
       " (395, 399): 1,\n",
       " (718, 729): 7,\n",
       " (729, 740): 7,\n",
       " (740, 748): 7,\n",
       " (748, 749): 7,\n",
       " (749, 750): 7,\n",
       " (750, 718): 7,\n",
       " (740, 751): 7,\n",
       " (751, 752): 7,\n",
       " (752, 753): 7,\n",
       " (753, 719): 7,\n",
       " (719, 748): 7,\n",
       " (752, 720): 1,\n",
       " (720, 721): 1,\n",
       " (721, 722): 1,\n",
       " (722, 723): 7,\n",
       " (723, 753): 7,\n",
       " (723, 724): 7,\n",
       " (724, 725): 7,\n",
       " (725, 726): 7,\n",
       " (726, 719): 7,\n",
       " (722, 727): 7,\n",
       " (727, 728): 7,\n",
       " (728, 730): 7,\n",
       " (730, 724): 7,\n",
       " (751, 731): 1,\n",
       " (731, 732): 2,\n",
       " (731, 733): 2,\n",
       " (718, 734): 1,\n",
       " (729, 735): 1,\n",
       " (749, 736): 1,\n",
       " (750, 737): 1,\n",
       " (725, 738): 1,\n",
       " (726, 739): 1,\n",
       " (727, 741): 1,\n",
       " (728, 742): 1,\n",
       " (730, 743): 1,\n",
       " (720, 744): 1,\n",
       " (720, 745): 1,\n",
       " (721, 746): 1,\n",
       " (721, 747): 1,\n",
       " (440, 451): 7,\n",
       " (451, 462): 7,\n",
       " (462, 464): 7,\n",
       " (464, 465): 7,\n",
       " (465, 466): 7,\n",
       " (466, 440): 7,\n",
       " (462, 467): 7,\n",
       " (467, 468): 7,\n",
       " (468, 469): 7,\n",
       " (469, 441): 7,\n",
       " (441, 464): 7,\n",
       " (468, 442): 1,\n",
       " (442, 443): 1,\n",
       " (443, 444): 1,\n",
       " (444, 445): 2,\n",
       " (445, 469): 1,\n",
       " (441, 446): 1,\n",
       " (446, 447): 2,\n",
       " (446, 448): 2,\n",
       " (440, 449): 1,\n",
       " (451, 450): 1,\n",
       " (465, 452): 1,\n",
       " (466, 453): 1,\n",
       " (467, 454): 1,\n",
       " (445, 455): 1,\n",
       " (444, 456): 1,\n",
       " (443, 457): 5,\n",
       " (443, 458): 1,\n",
       " (442, 459): 4,\n",
       " (442, 460): 1,\n",
       " (459, 461): 1,\n",
       " (457, 463): 1,\n",
       " (470, 481): 1,\n",
       " (481, 492): 1,\n",
       " (492, 502): 7,\n",
       " (502, 503): 1,\n",
       " (503, 504): 1,\n",
       " (504, 470): 1,\n",
       " (492, 505): 7,\n",
       " (505, 506): 7,\n",
       " (506, 507): 7,\n",
       " (507, 471): 7,\n",
       " (471, 502): 7,\n",
       " (506, 472): 7,\n",
       " (472, 473): 7,\n",
       " (473, 474): 7,\n",
       " (474, 475): 7,\n",
       " (475, 507): 7,\n",
       " (475, 476): 7,\n",
       " (476, 477): 7,\n",
       " (477, 478): 7,\n",
       " (478, 471): 7,\n",
       " (474, 479): 7,\n",
       " (479, 480): 7,\n",
       " (480, 482): 7,\n",
       " (482, 476): 7,\n",
       " (505, 483): 1,\n",
       " (483, 484): 2,\n",
       " (483, 485): 2,\n",
       " (503, 486): 1,\n",
       " (503, 487): 1,\n",
       " (504, 488): 1,\n",
       " (504, 489): 1,\n",
       " (472, 490): 1,\n",
       " (473, 491): 1,\n",
       " (477, 493): 1,\n",
       " (478, 494): 1,\n",
       " (479, 495): 1,\n",
       " (480, 496): 1,\n",
       " (482, 497): 1,\n",
       " (470, 498): 1,\n",
       " (470, 499): 1,\n",
       " (481, 500): 1,\n",
       " (481, 501): 1,\n",
       " (508, 519): 1,\n",
       " (519, 530): 1,\n",
       " (530, 538): 7,\n",
       " (538, 539): 1,\n",
       " (539, 540): 2,\n",
       " (540, 508): 1,\n",
       " (530, 541): 7,\n",
       " (541, 542): 7,\n",
       " (542, 543): 7,\n",
       " (543, 509): 7,\n",
       " (509, 538): 7,\n",
       " (542, 510): 7,\n",
       " (510, 511): 7,\n",
       " (511, 512): 7,\n",
       " (512, 513): 7,\n",
       " (513, 543): 7,\n",
       " (512, 514): 7,\n",
       " (514, 515): 7,\n",
       " (515, 516): 7,\n",
       " (516, 517): 7,\n",
       " (517, 513): 7,\n",
       " (541, 518): 1,\n",
       " (518, 520): 2,\n",
       " (518, 521): 2,\n",
       " (509, 522): 1,\n",
       " (510, 523): 1,\n",
       " (511, 524): 1,\n",
       " (514, 525): 1,\n",
       " (515, 526): 1,\n",
       " (516, 527): 1,\n",
       " (517, 528): 1,\n",
       " (539, 529): 1,\n",
       " (540, 531): 1,\n",
       " (508, 532): 5,\n",
       " (508, 533): 1,\n",
       " (519, 534): 4,\n",
       " (519, 535): 1,\n",
       " (532, 536): 1,\n",
       " (534, 537): 1,\n",
       " (544, 555): 7,\n",
       " (555, 566): 7,\n",
       " (566, 574): 7,\n",
       " (574, 575): 7,\n",
       " (575, 576): 7,\n",
       " (576, 544): 7,\n",
       " (566, 577): 7,\n",
       " (577, 578): 7,\n",
       " (578, 579): 7,\n",
       " (579, 545): 7,\n",
       " (545, 574): 7,\n",
       " (578, 546): 7,\n",
       " (546, 547): 7,\n",
       " (547, 548): 7,\n",
       " (548, 549): 7,\n",
       " (549, 579): 7,\n",
       " (548, 550): 1,\n",
       " (550, 551): 1,\n",
       " (551, 552): 1,\n",
       " (552, 553): 2,\n",
       " (553, 549): 1,\n",
       " (577, 554): 1,\n",
       " (554, 556): 2,\n",
       " (554, 557): 2,\n",
       " (544, 558): 1,\n",
       " (555, 559): 1,\n",
       " (575, 560): 1,\n",
       " (576, 561): 1,\n",
       " (545, 562): 1,\n",
       " (546, 563): 1,\n",
       " (547, 564): 1,\n",
       " (551, 565): 5,\n",
       " (551, 567): 1,\n",
       " (550, 568): 4,\n",
       " (550, 569): 1,\n",
       " (565, 570): 1,\n",
       " (568, 571): 1,\n",
       " (552, 572): 1,\n",
       " (553, 573): 1,\n",
       " (580, 591): 1,\n",
       " (591, 602): 7,\n",
       " (602, 606): 1,\n",
       " (606, 607): 7,\n",
       " (607, 608): 1,\n",
       " (608, 580): 7,\n",
       " (606, 609): 7,\n",
       " (609, 610): 7,\n",
       " (610, 611): 7,\n",
       " (611, 581): 7,\n",
       " (581, 607): 7,\n",
       " (609, 582): 1,\n",
       " (610, 583): 1,\n",
       " (611, 584): 1,\n",
       " (608, 585): 7,\n",
       " (585, 586): 7,\n",
       " (586, 587): 7,\n",
       " (587, 588): 7,\n",
       " (588, 580): 7,\n",
       " (585, 589): 1,\n",
       " (586, 590): 1,\n",
       " (587, 592): 1,\n",
       " (588, 593): 1,\n",
       " (591, 594): 7,\n",
       " (594, 595): 7,\n",
       " (595, 596): 7,\n",
       " (596, 597): 7,\n",
       " (597, 602): 7,\n",
       " (594, 598): 1,\n",
       " (595, 599): 1,\n",
       " (596, 600): 1,\n",
       " (597, 601): 1,\n",
       " (581, 603): 1,\n",
       " (603, 604): 2,\n",
       " (603, 605): 2,\n",
       " (612, 623): 1,\n",
       " (623, 634): 1,\n",
       " (634, 645): 7,\n",
       " (645, 646): 1,\n",
       " (646, 647): 1,\n",
       " (647, 612): 1,\n",
       " (634, 648): 7,\n",
       " (648, 649): 7,\n",
       " (649, 650): 7,\n",
       " (650, 613): 7,\n",
       " (613, 645): 7,\n",
       " (649, 614): 7,\n",
       " (614, 615): 7,\n",
       " (615, 616): 7,\n",
       " (616, 617): 7,\n",
       " (617, 650): 7,\n",
       " (617, 618): 7,\n",
       " (618, 619): 7,\n",
       " (619, 620): 7,\n",
       " (620, 613): 7,\n",
       " (616, 621): 7,\n",
       " (621, 622): 7,\n",
       " (622, 624): 7,\n",
       " (624, 618): 7,\n",
       " (648, 625): 1,\n",
       " (625, 626): 2,\n",
       " (625, 627): 2,\n",
       " (646, 628): 1,\n",
       " (646, 629): 1,\n",
       " (647, 630): 1,\n",
       " (647, 631): 1,\n",
       " (614, 632): 1,\n",
       " (615, 633): 1,\n",
       " (619, 635): 1,\n",
       " (620, 636): 1,\n",
       " (621, 637): 1,\n",
       " (622, 638): 1,\n",
       " (624, 639): 1,\n",
       " (612, 640): 1,\n",
       " (612, 641): 1,\n",
       " (623, 642): 1,\n",
       " (623, 643): 1,\n",
       " (642, 644): 1,\n",
       " (651, 662): 7,\n",
       " (662, 673): 7,\n",
       " (673, 678): 7,\n",
       " (678, 679): 7,\n",
       " (679, 680): 7,\n",
       " (680, 651): 7,\n",
       " (673, 681): 7,\n",
       " (681, 682): 7,\n",
       " (682, 683): 7,\n",
       " (683, 652): 7,\n",
       " (652, 678): 7,\n",
       " (682, 653): 7,\n",
       " (653, 654): 7,\n",
       " (654, 655): 7,\n",
       " (655, 656): 7,\n",
       " (656, 683): 7,\n",
       " (655, 657): 7,\n",
       " (657, 658): 7,\n",
       " (658, 659): 7,\n",
       " (659, 660): 7,\n",
       " (660, 656): 7,\n",
       " (679, 661): 1,\n",
       " (651, 663): 1,\n",
       " (662, 664): 1,\n",
       " (680, 665): 1,\n",
       " (652, 666): 1,\n",
       " (653, 667): 1,\n",
       " (654, 668): 1,\n",
       " (657, 669): 1,\n",
       " (658, 670): 1,\n",
       " (659, 671): 1,\n",
       " (660, 672): 1,\n",
       " (661, 674): 1,\n",
       " (681, 675): 1,\n",
       " (675, 676): 2,\n",
       " (675, 677): 2,\n",
       " (684, 695): 7,\n",
       " (695, 706): 7,\n",
       " (706, 712): 7,\n",
       " (712, 713): 7,\n",
       " (713, 714): 7,\n",
       " (714, 684): 7,\n",
       " (684, 715): 1,\n",
       " (695, 716): 1,\n",
       " (714, 717): 1,\n",
       " (712, 685): 7,\n",
       " (685, 686): 7,\n",
       " (686, 687): 7,\n",
       " (687, 688): 7,\n",
       " (688, 713): 7,\n",
       " (686, 689): 1,\n",
       " (687, 690): 1,\n",
       " (688, 691): 1,\n",
       " (706, 692): 1,\n",
       " (692, 693): 7,\n",
       " (693, 694): 7,\n",
       " (694, 685): 1,\n",
       " (693, 696): 7,\n",
       " (696, 697): 7,\n",
       " (697, 698): 7,\n",
       " (698, 699): 7,\n",
       " (699, 694): 7,\n",
       " (697, 700): 1,\n",
       " (698, 701): 1,\n",
       " (699, 702): 1,\n",
       " (692, 703): 7,\n",
       " (703, 704): 7,\n",
       " (704, 705): 7,\n",
       " (705, 696): 7,\n",
       " (703, 707): 1,\n",
       " (704, 708): 1,\n",
       " (705, 709): 1,\n",
       " (709, 710): 2,\n",
       " (709, 711): 2,\n",
       " (754, 765): 7,\n",
       " (765, 770): 7,\n",
       " (770, 771): 7,\n",
       " (771, 772): 7,\n",
       " (772, 773): 7,\n",
       " (773, 754): 7,\n",
       " (754, 774): 1,\n",
       " (765, 775): 1,\n",
       " (772, 776): 1,\n",
       " (770, 755): 1,\n",
       " (755, 756): 1,\n",
       " (771, 757): 1,\n",
       " (757, 756): 1,\n",
       " (755, 758): 1,\n",
       " (758, 759): 1,\n",
       " (758, 760): 1,\n",
       " (758, 761): 1,\n",
       " (773, 762): 1,\n",
       " (762, 763): 2,\n",
       " (762, 764): 2,\n",
       " (757, 766): 1,\n",
       " (757, 767): 1,\n",
       " (756, 768): 1,\n",
       " (756, 769): 1,\n",
       " (777, 788): 7,\n",
       " (788, 791): 7,\n",
       " (791, 792): 7,\n",
       " (792, 793): 7,\n",
       " (793, 794): 7,\n",
       " (794, 777): 7,\n",
       " (791, 795): 7,\n",
       " (795, 796): 7,\n",
       " (792, 797): 7,\n",
       " (797, 796): 7,\n",
       " (795, 778): 1,\n",
       " (794, 779): 1,\n",
       " (779, 780): 2,\n",
       " (779, 781): 2,\n",
       " (777, 782): 1,\n",
       " (788, 783): 1,\n",
       " (793, 784): 1,\n",
       " (796, 785): 1,\n",
       " (797, 786): 1,\n",
       " (778, 787): 1,\n",
       " (778, 789): 1,\n",
       " (778, 790): 1,\n",
       " (798, 804): 7,\n",
       " (804, 805): 7,\n",
       " (805, 806): 7,\n",
       " (806, 807): 7,\n",
       " (807, 808): 7,\n",
       " (808, 798): 7,\n",
       " (798, 809): 1,\n",
       " (804, 810): 1,\n",
       " (805, 811): 1,\n",
       " (806, 799): 1,\n",
       " (808, 800): 1,\n",
       " (807, 801): 1,\n",
       " (801, 802): 2,\n",
       " (801, 803): 2,\n",
       " (812, 823): 7,\n",
       " (823, 826): 7,\n",
       " (826, 827): 7,\n",
       " (827, 828): 7,\n",
       " (828, 829): 7,\n",
       " (829, 812): 7,\n",
       " (826, 830): 7,\n",
       " (830, 831): 7,\n",
       " (827, 832): 7,\n",
       " (832, 831): 7,\n",
       " (830, 813): 1,\n",
       " (823, 814): 1,\n",
       " (814, 815): 2,\n",
       " (814, 816): 2,\n",
       " (812, 817): 1,\n",
       " (828, 818): 1,\n",
       " (829, 819): 1,\n",
       " (831, 820): 1,\n",
       " (832, 821): 1,\n",
       " (813, 822): 1,\n",
       " (813, 824): 1,\n",
       " (813, 825): 1,\n",
       " (833, 843): 7,\n",
       " (843, 844): 7,\n",
       " (844, 845): 7,\n",
       " (845, 846): 7,\n",
       " (846, 847): 7,\n",
       " (847, 833): 7,\n",
       " (844, 848): 7,\n",
       " (848, 849): 7,\n",
       " (845, 850): 7,\n",
       " (850, 849): 7,\n",
       " (846, 834): 1,\n",
       " (834, 835): 2,\n",
       " (834, 836): 2,\n",
       " (833, 837): 1,\n",
       " (843, 838): 1,\n",
       " (847, 839): 1,\n",
       " (849, 840): 1,\n",
       " (850, 841): 1,\n",
       " (848, 842): 1,\n",
       " (851, 860): 7,\n",
       " (860, 861): 7,\n",
       " (861, 862): 7,\n",
       " (862, 863): 7,\n",
       " (863, 864): 7,\n",
       " (864, 851): 7,\n",
       " (861, 865): 7,\n",
       " (865, 866): 7,\n",
       " (862, 867): 7,\n",
       " (867, 866): 7,\n",
       " (864, 852): 1,\n",
       " (852, 853): 2,\n",
       " (852, 854): 2,\n",
       " (865, 855): 1,\n",
       " (851, 856): 1,\n",
       " (860, 857): 1,\n",
       " (863, 858): 1,\n",
       " (867, 859): 1,\n",
       " (868, 874): 7,\n",
       " (874, 875): 7,\n",
       " (875, 876): 7,\n",
       " (876, 877): 7,\n",
       " (877, 878): 7,\n",
       " (878, 868): 7,\n",
       " (868, 879): 1,\n",
       " (874, 880): 1,\n",
       " (875, 881): 1,\n",
       " (878, 869): 1,\n",
       " (877, 870): 1,\n",
       " (876, 871): 1,\n",
       " (870, 872): 2,\n",
       " (870, 873): 2,\n",
       " (882, 887): 7,\n",
       " (887, 888): 7,\n",
       " (888, 889): 7,\n",
       " (889, 890): 7,\n",
       " (890, 882): 7,\n",
       " (887, 891): 1,\n",
       " (888, 892): 1,\n",
       " (889, 893): 1,\n",
       " (893, 894): 2,\n",
       " (893, 883): 2,\n",
       " (882, 884): 1,\n",
       " (884, 885): 2,\n",
       " (884, 886): 1,\n",
       " (895, 905): 7,\n",
       " (905, 906): 7,\n",
       " (906, 907): 7,\n",
       " (907, 908): 7,\n",
       " (908, 895): 7,\n",
       " (905, 909): 1,\n",
       " (906, 910): 1,\n",
       " (908, 911): 1,\n",
       " (895, 912): 1,\n",
       " (912, 896): 2,\n",
       " (912, 897): 2,\n",
       " (898, 899): 2,\n",
       " (899, 900): 1,\n",
       " (900, 901): 2,\n",
       " (900, 902): 1,\n",
       " (899, 903): 1,\n",
       " (907, 898): 1,\n",
       " (898, 904): 1,\n",
       " (913, 924): 7,\n",
       " (924, 926): 7,\n",
       " (926, 927): 7,\n",
       " (927, 928): 7,\n",
       " (928, 913): 7,\n",
       " (924, 929): 1,\n",
       " (926, 930): 1,\n",
       " (927, 931): 1,\n",
       " (931, 932): 2,\n",
       " (931, 914): 2,\n",
       " (913, 915): 1,\n",
       " (915, 916): 2,\n",
       " (915, 917): 1,\n",
       " (916, 918): 1,\n",
       " (918, 919): 1,\n",
       " (918, 920): 1,\n",
       " (919, 921): 2,\n",
       " (919, 922): 1,\n",
       " (922, 923): 1,\n",
       " (922, 925): 1,\n",
       " (933, 944): 7,\n",
       " (944, 951): 7,\n",
       " (951, 952): 7,\n",
       " (952, 953): 7,\n",
       " (953, 933): 7,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_data():\n",
    "    data = []\n",
    "    for sample_id in tqdm(molecule_ids):\n",
    "        # 首先获取节点信息\n",
    "        nodes = atom[\"id\"].loc[atom[\"drug_id\"] == sample_id].tolist()\n",
    "        nodes = sorted(nodes)\n",
    "\n",
    "        # 构建邻接矩阵\n",
    "        adj_matrix = np.zeros((len(nodes), len(nodes)))\n",
    "        # adj_matrix[-1, :-1] = 1  # 构造一个超级节点\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(len(nodes)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                if (nodes[i], nodes[j]) in edges_dict:\n",
    "                    adj_matrix[i][j] = edges_dict[nodes[i], nodes[j]]\n",
    "        # 获取前全局特征\n",
    "        ind1 = molecule[\"ind1\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        inda = molecule[\"inda\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        logp = molecule[\"logp\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        lumo = molecule[\"lumo\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        global_feature = {\"ind1\": ind1, \"inda\": inda, \"logp\": logp, \"lumo\": lumo}\n",
    "        # 获取每个节点的特征\n",
    "        node_features = []\n",
    "        mean_charge = 0\n",
    "        for node in nodes:\n",
    "            node_type = atom[\"atom_type\"].loc[atom[\"id\"] == node].tolist()[0]\n",
    "            node_charge = atom[\"charge\"].loc[atom[\"id\"] == node].tolist()[0]\n",
    "            mean_charge += node_charge\n",
    "            node_feature = {\"type\": node_type, \"charge\": node_charge}\n",
    "            node_features.append(node_feature)\n",
    "        # mean_charge /= len(nodes)\n",
    "        # node_features.append({\"type\": 1, \"charge\": mean_charge})  # 超级节点\n",
    "        # node\n",
    "        sample = dict()\n",
    "        sample[\"drug_id\"] = sample_id\n",
    "        nodes = np.array(nodes) - nodes[0]\n",
    "        nodes = nodes.astype(np.int64).tolist()\n",
    "        # nodes.append(len(nodes) - 1)\n",
    "        sample[\"nodes\"] = nodes\n",
    "        sample[\"adj_matrix\"] = adj_matrix\n",
    "        sample[\"global_features\"] = global_feature\n",
    "        sample[\"node_features\"] = node_features\n",
    "        sample[\"label\"] = (\n",
    "            molecule[\"active\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        )\n",
    "        data.append(sample)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 44.31it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_molecule_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 7., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 7., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 7., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 7., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 2., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2.,\n",
       "        2., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0., 2., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 2., 2., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 7., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 7., 0., 0., 0., 0., 7.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 7., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 7., 0., 0., 0.],\n",
       "       [7., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 7., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"adj_matrix\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super(Phi, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, embed_dim)\n",
    "        self.ind1_embedding = nn.Embedding(num_ind1_type, embed_dim)\n",
    "        self.inda_embedding = nn.Embedding(num_inda_type, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, l_n, l_n_u, l_u):\n",
    "        \"\"\"transition network，用于生成转换矩阵 ${A}_{n,u}$\n",
    "            转换矩阵的作用是\n",
    "        Args:\n",
    "            l_n (_type_): 节点的特征 包括：\n",
    "                1. atom_type class\n",
    "                2. charge float\n",
    "                3. ind1 class\n",
    "                4. inda class\n",
    "                5. logp float\n",
    "                6. lumo float\n",
    "            l_n_u (): 边的特征\n",
    "            l_u (_type_): 邻居节点的特征\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class HwNonLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        num_edge_type: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        \"\"\"非线性的GNN的H_w函数实现，在利用节点、边、邻居节点的特征构造相关向量，\n",
    "        然后拼接/加和节点状态向量，使用三层的FNN网络计算新的节点状态。\n",
    "        Args:\n",
    "            num_atom_type (int): 原子的种类数量\n",
    "            num_ind1_type (int): ind1的种类数量\n",
    "            num_inda_type (int): inda的种类数量\n",
    "            embed_dim (int): 嵌入的维度\n",
    "        \"\"\"\n",
    "        super(HwNonLinear, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, embed_dim)\n",
    "        self.ind1_embedding = nn.Embedding(num_ind1_type, embed_dim)\n",
    "        self.inda_embedding = nn.Embedding(num_inda_type, embed_dim)\n",
    "        self.edge_embedding = nn.Embedding(num_edge_type + 1, embed_dim)\n",
    "        self.logp_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.lumo_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.charge_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.fnn_1 = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fnn_2 = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, l_n, adj_matrix):\n",
    "        \"\"\"前向传播，根据节点原来的状态向量、节点的特征l_n、邻居节点的特征l_u、边的特征l_{n,u}\n",
    "        计算新的节点状态向量。为了实现高效的计算，使用批量计算的方式，并且使用平均的方式进行融合。\n",
    "        传入的l_n是一张图的所有节点的特征，邻居节点的特征也可以从中获取，邻居可以有邻接矩阵得到。\n",
    "        1. 首先对边进行嵌入，直接使用邻接矩阵进行嵌入，将0也看作一个类别，得到一个 n x n x e的张量 \\n\n",
    "        2. 然后对节点进行嵌入，对每个都进行嵌入，然后加和得到一个 n x e的张量 \\n\n",
    "        3. 将节点特征和边特征进行加和，得到一个 n x n x e 的张量 \\n\n",
    "        4. 根据邻接矩阵得到mask矩阵，根据mask矩阵在第二个维度上求和，得到 n x e的张量 \\n\n",
    "        5. 将上述得到的向量和x拼接，得到一个 n x (2e)的张量 \\n\n",
    "        6. 经过一个三层的FNN网络，得到一个 n x e的张量，即新的节点状态向量 \\n\n",
    "        Args:\n",
    "            l_n (_type_): 节点的特征 包括：\n",
    "                dict:\n",
    "                1. atom_type class [n]\n",
    "                2. charge float [n]\n",
    "                3. ind1 class [n]\n",
    "                4. inda class [n]\n",
    "                5. logp float [n]\n",
    "                6. lumo float [n]\n",
    "            adj_matrix (): 邻接矩阵\n",
    "\n",
    "        Returns:\n",
    "            _type_: 新的节点状态向量\n",
    "        \"\"\"\n",
    "        atom_embed = self.atom_embedding(l_n[\"atom_type\"])  # [n,e]\n",
    "        ind1_embed = self.ind1_embedding(l_n[\"ind1\"])\n",
    "        inda_embed = self.inda_embedding(l_n[\"inda\"])\n",
    "        logp_embed = self.logp_embedding * l_n[\"logp\"].unsqueeze(dim=-1)\n",
    "        lumo_embed = self.lumo_embedding * l_n[\"lumo\"].unsqueeze(dim=-1)  # [n,e]\n",
    "        charge_embed = self.charge_embedding * l_n[\"charge\"].unsqueeze(dim=-1)\n",
    "        # 计算节点特征\n",
    "        node_embed = (\n",
    "            atom_embed\n",
    "            + ind1_embed\n",
    "            + inda_embed\n",
    "            + logp_embed\n",
    "            + lumo_embed\n",
    "            + charge_embed\n",
    "        ) / 6  # [n,e]\n",
    "        # 计算边特征\n",
    "        edge_embed = self.edge_embedding(adj_matrix)  # [n,n,e]\n",
    "        # 计算节点特征和边特征的加和\n",
    "        # 计算mask矩阵\n",
    "        mask = adj_matrix.unsqueeze(dim=-1) != 0  # [n,n,1]\n",
    "        mask = mask.float()\n",
    "        # 计算mask矩阵在第二个维度上的和\n",
    "        edge_embed = torch.sum(edge_embed * mask, dim=1) / (\n",
    "            mask.squeeze().sum(dim=1, keepdim=True) + 1e-10\n",
    "        )  # [n,e]\n",
    "        # 拼接节点特征和边特征\n",
    "        # x = torch.cat([x, node_edge_embed], dim=-1)  # [n,2e]\n",
    "        node_agg = (mask.squeeze() @ (x + node_embed)) / (\n",
    "            mask.squeeze().sum(dim=1, keepdim=True) + 1e-10\n",
    "        )\n",
    "        x = (node_agg + edge_embed) / 2\n",
    "        # 经过一个三层的FNN网络\n",
    "        x = torch.tanh(self.fnn_1(x))  # [n,e]\n",
    "        # x = torch.tanh(self.fnn_2(x))  # [n,e]\n",
    "        return x\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        num_edge_type: int,\n",
    "        t,\n",
    "    ):\n",
    "        super(GNN, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.t = t\n",
    "        self.hw = HwNonLinear(\n",
    "            num_atom_type, num_ind1_type, num_inda_type, num_edge_type, embed_dim\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, 1),\n",
    "        )\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.l2_reg = nn.MSELoss()\n",
    "\n",
    "    def contraction_penalty(self, params, threshold=0.9):\n",
    "        \"\"\"计算收缩映射的惩罚项\"\"\"\n",
    "        penalty = 0\n",
    "        for param in params:\n",
    "            # 计算参数的范数\n",
    "            norm = torch.norm(param, p=2)\n",
    "            # 如果范数大于阈值，则添加惩罚项\n",
    "            penalty += torch.pow(torch.relu(norm - threshold), 2)\n",
    "        return penalty\n",
    "\n",
    "    def forward(self, l_n, adj_matrix, label):\n",
    "        x = torch.zeros(adj_matrix.shape[0], self.embed_dim, requires_grad=False)\n",
    "        constraint_loss = 0\n",
    "        for i in range(self.t):\n",
    "            x2 = self.hw(x, l_n, adj_matrix)\n",
    "            constraint_loss += torch.norm(x2 - x, p=1)\n",
    "            x = x2\n",
    "        constraint_loss = 0\n",
    "        # x = x.sum(dim=0)\n",
    "        index = adj_matrix.sum(dim=1).argmax()\n",
    "        logits = self.output_layer(x[index])\n",
    "        label[label == 0] = -1\n",
    "        # loss = self.criterion(logits, label.float())\n",
    "        hw_params = self.hw.get_parameters()\n",
    "        penalty = self.contraction_penalty(hw_params, threshold=1)\n",
    "        loss = (\n",
    "            F.mse_loss(logits, label.float())\n",
    "            + penalty * 0.1\n",
    "            + constraint_loss / self.t\n",
    "            + 0\n",
    "        )\n",
    "        return logits, loss\n",
    "        # return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, device):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        nodes = sample[\"nodes\"]\n",
    "        adj_matrix = sample[\"adj_matrix\"]\n",
    "        label = sample[\"label\"]\n",
    "        atom_type = [t[\"type\"] for t in sample[\"node_features\"]]\n",
    "        charge = [t[\"charge\"] for t in sample[\"node_features\"]]\n",
    "        ind1 = [sample[\"global_features\"][\"ind1\"]] * len(nodes)\n",
    "        inda = [sample[\"global_features\"][\"inda\"]] * len(nodes)\n",
    "        logp = [sample[\"global_features\"][\"logp\"]] * len(nodes)\n",
    "        lumo = [sample[\"global_features\"][\"lumo\"]] * len(nodes)\n",
    "        l_n = {\n",
    "            \"atom_type\": torch.LongTensor(atom_type).to(self.device),\n",
    "            \"charge\": torch.tensor(charge).to(self.device),\n",
    "            \"ind1\": torch.LongTensor(ind1).to(self.device),\n",
    "            \"inda\": torch.LongTensor(inda).to(self.device),\n",
    "            \"logp\": torch.tensor(logp).to(self.device),\n",
    "            \"lumo\": torch.tensor(lumo).to(self.device),\n",
    "        }\n",
    "        adj_matrix = torch.LongTensor(adj_matrix).to(self.device)\n",
    "        label = torch.tensor([label]).to(self.device)\n",
    "        return {\"l_n\": l_n, \"adj_matrix\": adj_matrix, \"label\": label}\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        l_n = [f[\"l_n\"] for f in batch]\n",
    "        adj_matrix = [f[\"adj_matrix\"] for f in batch]\n",
    "        label = [f[\"label\"] for f in batch]\n",
    "        return l_n, adj_matrix, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=100)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_data, device)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=MyDataset.collate_fn,\n",
    ")\n",
    "meta_data = {\n",
    "    \"num_atom_type\": 233,\n",
    "    \"num_ind1_type\": 2,\n",
    "    \"num_inda_type\": 2,\n",
    "    \"num_edge_type\": 8,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN(embed_dim=100, t=10, **meta_data)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataset, epochs):\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=MyDataset.collate_fn,\n",
    "    )\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs, gamma=0.7)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = {\n",
    "                \"l_n\": batch[0][0],\n",
    "                \"adj_matrix\": batch[1][0],\n",
    "                \"label\": batch[2][0],\n",
    "            }\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(**inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        # scheduler.step()\n",
    "        # print(f\"Epoch:{epoch+1}, Loss:{total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MyDataset(test_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "# acc_cal = Accuracy(task=\"binary\", threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_dataset):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=1, shuffle=False, collate_fn=MyDataset.collate_fn\n",
    "    )\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for batch in test_loader:\n",
    "        inputs = {\n",
    "            \"l_n\": batch[0][0],\n",
    "            \"adj_matrix\": batch[1][0],\n",
    "            \"label\": batch[2][0],\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(**inputs)\n",
    "        labels.append(inputs[\"label\"])\n",
    "        preds.append(logits)\n",
    "    labels = torch.stack(labels)\n",
    "    preds = torch.stack(preds)\n",
    "    labels = labels.view(-1)\n",
    "    preds = preds.view(-1)\n",
    "    # acc = acc_cal(preds, labels)\n",
    "    labels[labels == -1] = 0\n",
    "    preds = preds > 0\n",
    "    acc = (preds == labels).sum() / len(labels)\n",
    "    return acc, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_dataset):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=1, shuffle=False, collate_fn=MyDataset.collate_fn\n",
    "    )\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for batch in test_loader:\n",
    "        inputs = {\n",
    "            \"l_n\": batch[0][0],\n",
    "            \"adj_matrix\": batch[1][0],\n",
    "            \"label\": batch[2][0],\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(**inputs)\n",
    "        labels.append(inputs[\"label\"])\n",
    "        preds.append(torch.sigmoid(logits))\n",
    "    labels = torch.stack(labels)\n",
    "    preds = torch.stack(preds)\n",
    "    labels = labels.view(-1)\n",
    "    preds = preds.view(-1)\n",
    "    # acc = acc_cal(preds, labels)\n",
    "    # labels[labels==-1] = 0\n",
    "    preds = preds > 0.5\n",
    "    acc = (preds == labels).sum() / len(labels)\n",
    "    return acc, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, train_dataset, epochs=10)\n",
    "acc, preds, labels = eval(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7500)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False, False, False, False, False]), tensor([-1,  1, -1,  1, -1]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Phi(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super(Phi, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, embed_dim)\n",
    "        self.ind1_embedding = nn.Embedding(num_ind1_type, embed_dim)\n",
    "        self.inda_embedding = nn.Embedding(num_inda_type, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, l_n, l_n_u, l_u):\n",
    "        \"\"\"transition network，用于生成转换矩阵 ${A}_{n,u}$\n",
    "            转换矩阵的作用是\n",
    "        Args:\n",
    "            l_n (_type_): 节点的特征 包括：\n",
    "                1. atom_type class\n",
    "                2. charge float\n",
    "                3. ind1 class\n",
    "                4. inda class\n",
    "                5. logp float\n",
    "                6. lumo float\n",
    "            l_n_u (): 边的特征\n",
    "            l_u (_type_): 邻居节点的特征\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class HwNonLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        num_edge_type: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        \"\"\"非线性的GNN的H_w函数实现，在利用节点、边、邻居节点的特征构造相关向量，\n",
    "        然后拼接/加和节点状态向量，使用三层的FNN网络计算新的节点状态。\n",
    "        Args:\n",
    "            num_atom_type (int): 原子的种类数量\n",
    "            num_ind1_type (int): ind1的种类数量\n",
    "            num_inda_type (int): inda的种类数量\n",
    "            embed_dim (int): 嵌入的维度\n",
    "        \"\"\"\n",
    "        super(HwNonLinear, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, embed_dim)\n",
    "        self.ind1_embedding = nn.Embedding(num_ind1_type, embed_dim)\n",
    "        self.inda_embedding = nn.Embedding(num_inda_type, embed_dim)\n",
    "        self.edge_embedding = nn.Embedding(num_edge_type + 1, embed_dim)\n",
    "        self.logp_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.lumo_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.charge_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.trans = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self._init()\n",
    "\n",
    "    def _init(self):\n",
    "        \"\"\"初始化参数\"\"\"\n",
    "        nn.init.xavier_uniform_(self.atom_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.ind1_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.inda_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.edge_embedding.weight)\n",
    "\n",
    "    def forward(self, x, l_n, adj_matrix):\n",
    "        \"\"\"前向传播，根据节点原来的状态向量、节点的特征l_n、邻居节点的特征l_u、边的特征l_{n,u}\n",
    "        计算新的节点状态向量。为了实现高效的计算，使用批量计算的方式，并且使用平均的方式进行融合。\n",
    "        传入的l_n是一张图的所有节点的特征，邻居节点的特征也可以从中获取，邻居可以有邻接矩阵得到。\n",
    "        1. 首先对边进行嵌入，直接使用邻接矩阵进行嵌入，将0也看作一个类别，得到一个 n x n x e的张量 \\n\n",
    "        2. 然后对节点进行嵌入，对每个都进行嵌入，然后加和得到一个 n x e的张量 \\n\n",
    "        3. 将节点特征和边特征进行加和，得到一个 n x n x e 的张量 \\n\n",
    "        4. 根据邻接矩阵得到mask矩阵，根据mask矩阵在第二个维度上求和，得到 n x e的张量 \\n\n",
    "        5. 将上述得到的向量和x拼接，得到一个 n x (2e)的张量 \\n\n",
    "        6. 经过一个三层的FNN网络，得到一个 n x e的张量，即新的节点状态向量 \\n\n",
    "        Args:\n",
    "            l_n (_type_): 节点的特征 包括：\n",
    "                dict:\n",
    "                1. atom_type class [n]\n",
    "                2. charge float [n]\n",
    "                3. ind1 class [n]\n",
    "                4. inda class [n]\n",
    "                5. logp float [n]\n",
    "                6. lumo float [n]\n",
    "            adj_matrix (): 邻接矩阵\n",
    "\n",
    "        Returns:\n",
    "            _type_: 新的节点状态向量\n",
    "        \"\"\"\n",
    "        atom_embed = self.atom_embedding(l_n[\"atom_type\"])  # [n,e]\n",
    "        ind1_embed = self.ind1_embedding(l_n[\"ind1\"])\n",
    "        inda_embed = self.inda_embedding(l_n[\"inda\"])\n",
    "        logp_embed = self.logp_embedding * l_n[\"logp\"].unsqueeze(dim=-1)\n",
    "        lumo_embed = self.lumo_embedding * l_n[\"lumo\"].unsqueeze(dim=-1)  # [n,e]\n",
    "        charge_embed = self.charge_embedding * l_n[\"charge\"].unsqueeze(dim=-1)\n",
    "        l_n = (\n",
    "            atom_embed\n",
    "            + ind1_embed\n",
    "            + inda_embed\n",
    "            + logp_embed\n",
    "            + lumo_embed\n",
    "            + charge_embed\n",
    "        )\n",
    "        l_n_u = self.edge_embedding(adj_matrix)  # [n,n,e]\n",
    "        x_u = x\n",
    "        mask = adj_matrix.unsqueeze(dim=-1) != 0  # [n,n,1]\n",
    "        mask = mask.float()\n",
    "        x_list = []\n",
    "        for i in range(x.shape[0]):\n",
    "            l_n_i = l_n[i].unsqueeze(dim=0).repeat(x.shape[0], 1)  # [n,e]\n",
    "            s = l_n_i + l_n_u[i] + x_u + l_n  # [n,e]\n",
    "            s = self.trans(s)\n",
    "            s = s * mask[i]  # [n,e]\n",
    "            s = torch.sum(s, dim=0)\n",
    "            x_list.append(s)\n",
    "        x = torch.stack(x_list, dim=0)\n",
    "        return x\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        num_edge_type: int,\n",
    "        t,\n",
    "    ):\n",
    "        super(GNN, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.t = t\n",
    "        self.hw = HwNonLinear(\n",
    "            num_atom_type, num_ind1_type, num_inda_type, num_edge_type, embed_dim\n",
    "        )\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, 1),\n",
    "        )\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.l2_reg = nn.MSELoss()\n",
    "\n",
    "    def contraction_penalty(self, params, threshold=0.9):\n",
    "        \"\"\"计算收缩映射的惩罚项\"\"\"\n",
    "        penalty = 0\n",
    "        for param in params:\n",
    "            # 计算参数的范数\n",
    "            norm = torch.norm(param, p=2)\n",
    "            # 如果范数大于阈值，则添加惩罚项\n",
    "            penalty += torch.pow(torch.relu(norm - threshold), 2)\n",
    "        return penalty\n",
    "\n",
    "    def forward(self, l_n, adj_matrix, label):\n",
    "        x = torch.randn(adj_matrix.shape[0], self.embed_dim, requires_grad=False)\n",
    "        constraint_loss = 0\n",
    "        # x2 = self.hw(x, l_n, adj_matrix)\n",
    "        # while torch.norm(x2 - x, p=2) > 1e-5:\n",
    "        #     x = x2\n",
    "        for i in range(self.t):\n",
    "            x2 = self.hw(x, l_n, adj_matrix)\n",
    "            constraint_loss += torch.norm(x2 - x, p=1)\n",
    "            x = x2\n",
    "        constraint_loss = 0\n",
    "        x = x2.sum(dim=0)\n",
    "        logits = self.output_layer(x)\n",
    "        loss = self.criterion(logits, label.float())\n",
    "        hw_params = self.hw.get_parameters()\n",
    "        penalty = self.contraction_penalty(hw_params, threshold=1)\n",
    "        loss = self.criterion(logits, label.float())\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:37, 37.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:12, 36.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:54, 38.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [02:36, 39.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [03:10, 37.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [03:40, 35.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [04:10, 33.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [04:38, 31.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [05:06, 30.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [05:33, 33.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "acc_scores = []\n",
    "for train_index, test_index in tqdm(kf.split(data)):\n",
    "    model = GNN(embed_dim=100, t=10, **meta_data)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "    )\n",
    "    data_train = [data[i] for i in train_index]\n",
    "    data_test = [data[i] for i in test_index]\n",
    "    train_dataset = MyDataset(data_train, device)\n",
    "    train(model, optimizer, train_dataset, epochs=10)\n",
    "    test_dataset = MyDataset(data_test, device)\n",
    "    acc, _, _ = eval(model, test_dataset)\n",
    "    print(acc)\n",
    "    acc_scores.append(acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7000000029802322"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
