{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>element</th>\n",
       "      <th>atom_type</th>\n",
       "      <th>charge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>h</td>\n",
       "      <td>3</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  drug_id element  atom_type  charge\n",
       "0   0        0       c         22   -0.11\n",
       "1   1        0       c         22   -0.11\n",
       "2   2        0       c         22   -0.11\n",
       "3   3        0       c         22   -0.11\n",
       "4   4        0       h          3    0.15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom = pd.read_csv(\n",
    "    \"dataset/Mutagenesis-42/atoms.csv\",\n",
    "    delimiter=\";\",\n",
    ")\n",
    "bond = pd.read_csv(\"dataset/Mutagenesis-42/bonds.csv\", delimiter=\";\")\n",
    "molecule = pd.read_csv(\"dataset/Mutagenesis-42/drugs.csv\", delimiter=\";\")\n",
    "atom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ind1</th>\n",
       "      <th>inda</th>\n",
       "      <th>act</th>\n",
       "      <th>logp</th>\n",
       "      <th>lumo</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>2.29</td>\n",
       "      <td>-3.025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4.35</td>\n",
       "      <td>-2.138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-1.429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-1.478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ind1  inda   act  logp   lumo  active\n",
       "0   0   0.0   0.0 -0.70  2.29 -3.025       0\n",
       "1   1   0.0   0.0  0.57  2.13 -0.798       1\n",
       "2   2   1.0   0.0  0.77  4.35 -2.138       1\n",
       "3   3   1.0   0.0 -0.22  5.41 -1.429       0\n",
       "4   4   1.0   0.0 -0.22  5.41 -1.478       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>atom1_id</th>\n",
       "      <th>atom2_id</th>\n",
       "      <th>bond_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  drug_id  atom1_id  atom2_id  bond_type\n",
       "0   0        0         0        11          7\n",
       "1   1        0        11        19          7\n",
       "2   2        0        19        20          7\n",
       "3   3        0        20        21          7\n",
       "4   4        0        21        22          7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_ids = list(range(molecule.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_dict = bond.values.tolist()\n",
    "edges_dict = {(s[2], s[3]): s[4] for s in edges_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_data():\n",
    "    data = []\n",
    "    for sample_id in tqdm(molecule_ids):\n",
    "        # 首先获取节点信息\n",
    "        nodes = atom[\"id\"].loc[atom[\"drug_id\"] == sample_id].tolist()\n",
    "        nodes = sorted(nodes)\n",
    "\n",
    "        # 构建edge_list 和 edge_attr\n",
    "        edge_attr = bond[\"bond_type\"].loc[bond[\"drug_id\"] == sample_id].tolist()\n",
    "        edge_attr = edge_attr + edge_attr\n",
    "        source_nodes = bond[\"atom1_id\"].loc[bond[\"drug_id\"] == sample_id] - nodes[0]\n",
    "        target_nodes = bond[\"atom2_id\"].loc[bond[\"drug_id\"] == sample_id] - nodes[0]\n",
    "        source_nodes = source_nodes.tolist()\n",
    "        target_nodes = target_nodes.tolist()\n",
    "        edge_list = [source_nodes + target_nodes, target_nodes + source_nodes]\n",
    "\n",
    "        # 获取前全局特征\n",
    "        ind1 = molecule[\"ind1\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        inda = molecule[\"inda\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        logp = molecule[\"logp\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        lumo = molecule[\"lumo\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        global_feature = {\"ind1\": ind1, \"inda\": inda, \"logp\": logp, \"lumo\": lumo}\n",
    "        # 获取每个节点的特征\n",
    "        node_features = []\n",
    "        mean_charge = 0\n",
    "        for node in nodes:\n",
    "            node_type = atom[\"atom_type\"].loc[atom[\"id\"] == node].tolist()[0]\n",
    "            node_charge = atom[\"charge\"].loc[atom[\"id\"] == node].tolist()[0]\n",
    "            mean_charge += node_charge\n",
    "            node_feature = {\"type\": node_type, \"charge\": node_charge}\n",
    "            node_features.append(node_feature)\n",
    "        sample = dict()\n",
    "        sample[\"drug_id\"] = sample_id\n",
    "        nodes = np.array(nodes) - nodes[0]\n",
    "        nodes = nodes.astype(np.int64).tolist()\n",
    "        # nodes.append(len(nodes) - 1)\n",
    "        sample[\"nodes\"] = nodes\n",
    "        sample[\"edge_list\"] = edge_list\n",
    "        sample[\"edge_attr\"] = edge_attr\n",
    "        sample[\"global_features\"] = global_feature\n",
    "        sample[\"node_features\"] = node_features\n",
    "        sample[\"label\"] = (\n",
    "            molecule[\"active\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        )\n",
    "        data.append(sample)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 175.10it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_molecule_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, device):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        nodes = sample[\"nodes\"]\n",
    "        edge_list = sample[\"edge_list\"]\n",
    "        label = sample[\"label\"]\n",
    "        atom_type = [t[\"type\"] for t in sample[\"node_features\"]]\n",
    "        charge = [t[\"charge\"] for t in sample[\"node_features\"]]\n",
    "        ind1 = [sample[\"global_features\"][\"ind1\"]] * len(nodes)\n",
    "        inda = [sample[\"global_features\"][\"inda\"]] * len(nodes)\n",
    "        logp = [sample[\"global_features\"][\"logp\"]] * len(nodes)\n",
    "        lumo = [sample[\"global_features\"][\"lumo\"]] * len(nodes)\n",
    "        l_n = {\n",
    "            \"atom_type\": torch.LongTensor(atom_type).to(self.device),\n",
    "            \"charge\": torch.tensor(charge).to(self.device),\n",
    "            \"ind1\": torch.LongTensor(ind1).to(self.device),\n",
    "            \"inda\": torch.LongTensor(inda).to(self.device),\n",
    "            \"logp\": torch.tensor(logp).to(self.device),\n",
    "            \"lumo\": torch.tensor(lumo).to(self.device),\n",
    "        }\n",
    "        edge_list = torch.LongTensor(edge_list).to(self.device)\n",
    "        edge_attr = torch.LongTensor(sample[\"edge_attr\"]).to(self.device)\n",
    "        label = torch.tensor([label]).to(self.device)\n",
    "        num_nodes = len(nodes)\n",
    "        return {\n",
    "            \"num_nodes\": num_nodes,\n",
    "            \"node_labels\": l_n,\n",
    "            \"edge_list\": edge_list,\n",
    "            \"edge_attr\": edge_attr,\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        num_nodes = [sample[\"num_nodes\"] for sample in batch]\n",
    "        node_labels = [sample[\"node_labels\"] for sample in batch]\n",
    "        edges_list = [sample[\"edge_list\"] for sample in batch]\n",
    "        label = [sample[\"label\"] for sample in batch]\n",
    "        edge_attr = [sample[\"edge_attr\"] for sample in batch]\n",
    "        return num_nodes, node_labels, edges_list, edge_attr, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataset, epochs):\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=MyDataset.collate_fn,\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=1,\n",
    "        end_factor=0.1,\n",
    "        total_iters=epochs,\n",
    "    )\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs, gamma=0.7)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = {\n",
    "                \"num_nodes\": batch[0][0],\n",
    "                \"node_labels\": batch[1][0],\n",
    "                \"edge_list\": batch[2][0],\n",
    "                \"edge_attr\": batch[3][0],\n",
    "                \"label\": batch[4][0],\n",
    "            }\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(**inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        # print(f\"Epoch:{epoch+1}, Loss:{total_loss}\")\n",
    "\n",
    "\n",
    "def eval(model, test_dataset):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=1, shuffle=False, collate_fn=MyDataset.collate_fn\n",
    "    )\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for batch in test_loader:\n",
    "        inputs = {\n",
    "            \"num_nodes\": batch[0][0],\n",
    "            \"node_labels\": batch[1][0],\n",
    "            \"edge_list\": batch[2][0],\n",
    "            \"edge_attr\": batch[3][0],\n",
    "            \"label\": batch[4][0],\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(**inputs)\n",
    "        labels.append(inputs[\"label\"])\n",
    "        preds.append(torch.sigmoid(logits))\n",
    "    labels = torch.stack(labels)\n",
    "    preds = torch.stack(preds)\n",
    "    labels = labels.view(-1)\n",
    "    preds = preds.view(-1)\n",
    "    # acc = acc_cal(preds, labels)\n",
    "    # labels[labels==-1] = 0\n",
    "    preds = preds > 0.5\n",
    "    acc = (preds == labels).sum() / len(labels)\n",
    "    return acc, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=100)\n",
    "test_dataset = MyDataset(test_data, device)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset = MyDataset(train_data, device)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=MyDataset.collate_fn,\n",
    ")\n",
    "meta_data = {\n",
    "    \"num_atom_type\": 233,\n",
    "    \"num_ind1_type\": 2,\n",
    "    \"num_inda_type\": 2,\n",
    "    \"num_edge_type\": 8,\n",
    "}\n",
    "# model = GNN(embed_dim=100, t=10, **meta_data)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "# train(model, optimizer, train_dataset, epochs=10)\n",
    "# acc, preds, labels = eval(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super(NodeEncoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, embed_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, embed_dim)\n",
    "        self.ind1_embedding = nn.Embedding(num_ind1_type, embed_dim)\n",
    "        self.inda_embedding = nn.Embedding(num_inda_type, embed_dim)\n",
    "        self.logp_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.lumo_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.charge_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.trans = nn.Linear(embed_dim * 6, embed_dim)\n",
    "\n",
    "    def init(self):\n",
    "        nn.init.xavier_uniform_(self.atom_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.ind1_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.inda_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.logp_embedding)\n",
    "        nn.init.xavier_uniform_(self.lumo_embedding)\n",
    "        nn.init.xavier_uniform_(self.charge_embedding)\n",
    "\n",
    "    def forward(self, node_labels):\n",
    "        atom_embed = self.atom_embedding(node_labels[\"atom_type\"])  # [n,e]\n",
    "        ind1_embed = self.ind1_embedding(node_labels[\"ind1\"])\n",
    "        inda_embed = self.inda_embedding(node_labels[\"inda\"])\n",
    "        logp_embed = self.logp_embedding * node_labels[\"logp\"].unsqueeze(dim=-1)\n",
    "        lumo_embed = self.lumo_embedding * node_labels[\"lumo\"].unsqueeze(\n",
    "            dim=-1\n",
    "        )  # [n,e]\n",
    "        charge_embed = self.charge_embedding * node_labels[\"charge\"].unsqueeze(dim=-1)\n",
    "        node_features = (\n",
    "            atom_embed\n",
    "            + ind1_embed\n",
    "            + inda_embed\n",
    "            + logp_embed\n",
    "            + lumo_embed\n",
    "            + charge_embed\n",
    "        )\n",
    "        # node_features = torch.cat(\n",
    "        #     [\n",
    "        #         atom_embed,\n",
    "        #         ind1_embed,\n",
    "        #         inda_embed,\n",
    "        #         logp_embed,\n",
    "        #         lumo_embed,\n",
    "        #         charge_embed,\n",
    "        #     ],\n",
    "        #     dim=-1,\n",
    "        # )\n",
    "        # node_features = torch.tanh(self.trans(node_features))\n",
    "        return node_features  # [n,e]\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class EdgeEncoder(nn.Module):\n",
    "    def __init__(self, num_edge_type: int, embed_dim: int):\n",
    "        super(EdgeEncoder, self).__init__()\n",
    "        self.edge_embedding = nn.Embedding(num_edge_type + 1, embed_dim)\n",
    "        nn.init.xavier_uniform_(self.edge_embedding.weight)\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        edge_features = self.edge_embedding(edge_attr)  # [n,n,e]\n",
    "        return edge_features\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class HwNonLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        \"\"\"非线性的GNN的H_w函数实现，在利用节点、边、邻居节点的特征构造相关向量，\n",
    "        然后拼接/加和节点状态向量，使用三层的FNN网络计算新的节点状态。\n",
    "        Args:\n",
    "            num_atom_type (int): 原子的种类数量\n",
    "            num_ind1_type (int): ind1的种类数量\n",
    "            num_inda_type (int): inda的种类数量\n",
    "            embed_dim (int): 嵌入的维度\n",
    "        \"\"\"\n",
    "        super(HwNonLinear, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.trans = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            # nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.trans[0].weight)  # 初始化参数\n",
    "        nn.init.xavier_uniform_(self.trans[2].weight)  # 初始化参数\n",
    "        # nn.init.xavier_uniform_(self.trans[4].weight)  # 初始化参数\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_features,\n",
    "        edge_features,\n",
    "        neighbor_state,\n",
    "        neighbor_features,\n",
    "    ):\n",
    "        \"\"\"实现h_w函数，节点的特征l_n、边的特征l_nu、邻居节点的状态x_u、邻居节点的特征l_u，结合前馈神经网络进行前项传播。\n",
    "        Args:\n",
    "            node_features (torch.Tensor): l_n [n,e]\n",
    "            edge_features (torch.Tensor): l_nu [n,n,e]\n",
    "            neighbor_state (torch.Tensor): x_u [n,e]\n",
    "            neighbor_features (torch.Tensor): l_u [n,e]\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # x = self.trans(\n",
    "        #     node_features + edge_features + neighbor_state + neighbor_features\n",
    "        # )  # [m,e]\n",
    "        x = torch.cat(\n",
    "            [node_features, edge_features, neighbor_state, neighbor_features], dim=-1\n",
    "        )\n",
    "        x = self.trans(x)\n",
    "        return x\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class Aggr(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super(Aggr, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        edge_list,\n",
    "        num_node,\n",
    "    ):\n",
    "        \"\"\"聚合函数，聚合邻居节点的状态向量，得到新的节点状态向量。\n",
    "        Args:\n",
    "            x (torch.Tensor): 节点的状态向量 [n,e]\n",
    "            edge_list (torch.Tensor): 边的列表 [2,n]\n",
    "            num_node (int): 节点的数量\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        x_u = torch.index_select(x, 0, edge_list[1])  # [n,e]\n",
    "        x = torch.zeros(num_node, self.embed_dim).to(x.device)\n",
    "        for i in range(num_node):\n",
    "            x[i] = torch.sum(x_u[edge_list[0] == i], dim=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        num_edge_type: int,\n",
    "        t,\n",
    "    ):\n",
    "        super(GNN, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.t = t\n",
    "        self.node_encoder = NodeEncoder(\n",
    "            num_atom_type, num_ind1_type, num_inda_type, embed_dim\n",
    "        )\n",
    "        self.edge_encoder = EdgeEncoder(num_edge_type, embed_dim)\n",
    "        self.hw = HwNonLinear(embed_dim)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, 1),\n",
    "        )\n",
    "        self.aggr = Aggr(embed_dim)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.l2_reg = nn.MSELoss()\n",
    "        self.thresh = 1e-5\n",
    "\n",
    "    def contraction_penalty(self, params, threshold=0.9):\n",
    "        \"\"\"计算收缩映射的惩罚项\"\"\"\n",
    "        penalty = 0\n",
    "        for param in params:\n",
    "            # 计算参数的范数\n",
    "            norm = torch.norm(param, p=2)\n",
    "            # 如果范数大于阈值，则添加惩罚项\n",
    "            penalty += torch.pow(torch.relu(norm - threshold), 2)\n",
    "        return penalty\n",
    "\n",
    "    def forward(self, num_nodes, node_labels, edge_list, edge_attr, label):\n",
    "        \"\"\"前向传播\n",
    "        Args:\n",
    "            num_nodes (int): 节点的数量\n",
    "            node_labels (torch.Tensor): 节点的标签 [n,e]\n",
    "            edge_list (torch.Tensor): 边的列表 [2,n]\n",
    "            edge_attr (torch.Tensor): 边的属性 [n,n]\n",
    "            label (torch.Tensor): 标签 [1]\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            node_states = torch.zeros(num_nodes, self.embed_dim, requires_grad=True)\n",
    "        else:\n",
    "            node_states = torch.zeros(\n",
    "                num_nodes, self.embed_dim, requires_grad=True\n",
    "            )  # [n,e]\n",
    "        node_features = self.node_encoder(node_labels)  # [n,e]\n",
    "        edge_features = self.edge_encoder(edge_attr)  # [m,e]\n",
    "        l_n = torch.index_select(node_features, 0, edge_list[0])  # [m,e]\n",
    "        l_u = torch.index_select(node_features, 0, edge_list[1])\n",
    "        l_nu = edge_features\n",
    "        x_u = torch.index_select(node_states, 0, edge_list[1])\n",
    "        for i in range(self.t):\n",
    "            x = self.hw(l_n, l_nu, x_u, l_u)\n",
    "            new_state = self.aggr(x, edge_list, num_nodes)\n",
    "            with torch.no_grad():\n",
    "                distance = torch.norm(new_state - node_states, p=2, dim=-1)\n",
    "                check = distance < self.thresh\n",
    "            if check.all():\n",
    "                break\n",
    "            node_states = new_state\n",
    "            x_u = torch.index_select(node_states, 0, edge_list[1])\n",
    "        # logits = self.output_layer(node_states[0])\n",
    "        logits = self.output_layer(node_states.mean(dim=0))\n",
    "        hw_params = self.hw.get_parameters()\n",
    "        node_encoder_params = self.node_encoder.get_parameters()\n",
    "        edge_encoder_params = self.edge_encoder.get_parameters()\n",
    "        penalty = (\n",
    "            self.contraction_penalty(hw_params, threshold=1)\n",
    "            + self.contraction_penalty(node_encoder_params, threshold=1)\n",
    "            + self.contraction_penalty(edge_encoder_params, threshold=1)\n",
    "        )\n",
    "        loss = self.criterion(logits, label.float())\n",
    "        return logits, loss + penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super(NodeEncoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, embed_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, embed_dim)\n",
    "        self.ind1_embedding = nn.Embedding(num_ind1_type, embed_dim)\n",
    "        self.inda_embedding = nn.Embedding(num_inda_type, embed_dim)\n",
    "        self.logp_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.lumo_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.charge_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.trans = nn.Linear(embed_dim * 6, embed_dim)\n",
    "\n",
    "    def init(self):\n",
    "        nn.init.xavier_uniform_(self.atom_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.ind1_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.inda_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.logp_embedding)\n",
    "        nn.init.xavier_uniform_(self.lumo_embedding)\n",
    "        nn.init.xavier_uniform_(self.charge_embedding)\n",
    "\n",
    "    def forward(self, node_labels):\n",
    "        atom_embed = self.atom_embedding(node_labels[\"atom_type\"])  # [n,e]\n",
    "        ind1_embed = self.ind1_embedding(node_labels[\"ind1\"])\n",
    "        inda_embed = self.inda_embedding(node_labels[\"inda\"])\n",
    "        logp_embed = self.logp_embedding * node_labels[\"logp\"].unsqueeze(dim=-1)\n",
    "        lumo_embed = self.lumo_embedding * node_labels[\"lumo\"].unsqueeze(\n",
    "            dim=-1\n",
    "        )  # [n,e]\n",
    "        charge_embed = self.charge_embedding * node_labels[\"charge\"].unsqueeze(dim=-1)\n",
    "        node_features = (\n",
    "            atom_embed\n",
    "            # + ind1_embed\n",
    "            # + inda_embed\n",
    "            + logp_embed\n",
    "            + lumo_embed\n",
    "            + charge_embed\n",
    "        )\n",
    "        # node_features = torch.cat(\n",
    "        #     [\n",
    "        #         atom_embed,\n",
    "        #         ind1_embed,\n",
    "        #         inda_embed,\n",
    "        #         logp_embed,\n",
    "        #         lumo_embed,\n",
    "        #         charge_embed,\n",
    "        #     ],\n",
    "        #     dim=-1,\n",
    "        # )\n",
    "        # node_features = torch.tanh(self.trans(node_features))\n",
    "        return node_features  # [n,e]\n",
    "\n",
    "\n",
    "class EdgeEncoder(nn.Module):\n",
    "    def __init__(self, num_edge_type: int, embed_dim: int):\n",
    "        super(EdgeEncoder, self).__init__()\n",
    "        self.edge_embedding = nn.Embedding(num_edge_type + 1, embed_dim)\n",
    "        nn.init.xavier_uniform_(self.edge_embedding.weight)\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        edge_features = self.edge_embedding(edge_attr)  # [n,n,e]\n",
    "        return edge_features\n",
    "\n",
    "\n",
    "class Phi(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super(Phi, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.trans = nn.Linear(embed_dim, embed_dim * embed_dim)\n",
    "        self.mu = 0.5\n",
    "        self.s = 26\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        num_neighbors,\n",
    "    ):\n",
    "        # x [n,e]\n",
    "\n",
    "        mat = self.trans(x).view(-1, self.embed_dim, self.embed_dim)  # [n,e,e]\n",
    "        mat = (self.mu * mat) / (self.embed_dim * num_neighbors.unsqueeze(dim=-1))\n",
    "        return mat\n",
    "\n",
    "\n",
    "class Rou(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super(Rou, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.trans = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"实现Rou函数，使用线性层和Tanh激活函数进行前向传播。\n",
    "        Args:\n",
    "            x (torch.Tensor): 节点的状态向量 [n,e]\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        x = self.trans(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HwLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        \"\"\"非线性的GNN的H_w函数实现，在利用节点、边、邻居节点的特征构造相关向量，\n",
    "        然后拼接/加和节点状态向量，使用三层的FNN网络计算新的节点状态。\n",
    "        Args:\n",
    "            num_atom_type (int): 原子的种类数量\n",
    "            num_ind1_type (int): ind1的种类数量\n",
    "            num_inda_type (int): inda的种类数量\n",
    "            embed_dim (int): 嵌入的维度\n",
    "        \"\"\"\n",
    "        super(HwLinear, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.trans = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.trans[0].weight)  # 初始化参数\n",
    "        nn.init.xavier_uniform_(self.trans[2].weight)  # 初始化参数\n",
    "        nn.init.xavier_uniform_(self.trans[4].weight)  # 初始化参数\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_features,\n",
    "        edge_features,\n",
    "        neighbor_state,\n",
    "        neighbor_features,\n",
    "        a,\n",
    "        b,\n",
    "    ):\n",
    "        \"\"\"实现h_w函数，节点的特征l_n、边的特征l_nu、邻居节点的状态x_u、邻居节点的特征l_u，结合前馈神经网络进行前项传播。\n",
    "        Args:\n",
    "            node_features (torch.Tensor): l_n [n,e]\n",
    "            edge_features (torch.Tensor): l_nu [n,n,e]\n",
    "            neighbor_state (torch.Tensor): x_u [n,e]\n",
    "            neighbor_features (torch.Tensor): l_u [n,e]\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        x = node_features + edge_features + neighbor_state + neighbor_features\n",
    "        x = torch.einsum(\"ijk,ik->ij\", a, x) + b\n",
    "        return x\n",
    "\n",
    "\n",
    "class Aggr(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super(Aggr, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        edge_list,\n",
    "        num_node,\n",
    "    ):\n",
    "        \"\"\"聚合函数，聚合邻居节点的状态向量，得到新的节点状态向量。\n",
    "        Args:\n",
    "            x (torch.Tensor): 节点的状态向量 [n,e]\n",
    "            edge_list (torch.Tensor): 边的列表 [2,n]\n",
    "            num_node (int): 节点的数量\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        x_u = torch.index_select(x, 0, edge_list[1])  # [n,e]\n",
    "        x = torch.zeros_like(x)\n",
    "        for i in range(num_node):\n",
    "            x[i] = torch.sum(x_u[edge_list[0] == i], dim=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        num_edge_type: int,\n",
    "        t,\n",
    "    ):\n",
    "        super(GNN, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.t = t\n",
    "        self.node_encoder = NodeEncoder(\n",
    "            num_atom_type, num_ind1_type, num_inda_type, embed_dim\n",
    "        )\n",
    "        self.edge_encoder = EdgeEncoder(num_edge_type, embed_dim)\n",
    "        self.hw = HwLinear(embed_dim)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            # nn.Linear(embed_dim, embed_dim),\n",
    "            # nn.Tanh(),,\n",
    "            nn.Linear(embed_dim, 1),\n",
    "        )\n",
    "        self.aggr = Aggr(embed_dim)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.phi = Phi(embed_dim)\n",
    "        self.rou = Rou(embed_dim)\n",
    "\n",
    "    def forward(self, num_nodes, node_labels, edge_list, edge_attr, label):\n",
    "        \"\"\"前向传播\n",
    "        Args:\n",
    "            num_nodes (int): 节点的数量\n",
    "            node_labels (torch.Tensor): 节点的标签 [n,e]\n",
    "            edge_list (torch.Tensor): 边的列表 [2,n]\n",
    "            edge_attr (torch.Tensor): 边的属性 [n,n]\n",
    "            label (torch.Tensor): 标签 [1]\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        adj_list = torch.zeros(num_nodes, num_nodes)\n",
    "        adj_list[edge_list[0], edge_list[1]] = 1\n",
    "        num_neighbors = adj_list.sum(dim=1, keepdim=True)  # [n,1]\n",
    "        num_neighbors_select = torch.index_select(\n",
    "            num_neighbors, 0, edge_list[0]\n",
    "        )  # [m,1]\n",
    "        if self.training:\n",
    "            node_states = torch.zeros(num_nodes, self.embed_dim, requires_grad=True)\n",
    "        else:\n",
    "            node_states = torch.zeros(\n",
    "                num_nodes, self.embed_dim, requires_grad=True\n",
    "            )  # [n,e]\n",
    "        node_features = self.node_encoder(node_labels)  # [n,e]\n",
    "        edge_features = self.edge_encoder(edge_attr)  # [m,e]\n",
    "        l_n = torch.index_select(node_features, 0, edge_list[0])  # [m,e]\n",
    "        l_u = torch.index_select(node_features, 0, edge_list[1])\n",
    "        l_nu = edge_features\n",
    "        x_u = torch.index_select(node_states, 0, edge_list[1])\n",
    "        for i in range(self.t):\n",
    "            a = self.phi(l_n + l_nu + l_u + x_u, num_neighbors_select)\n",
    "            b = self.rou(l_n + l_nu + l_u + x_u)\n",
    "            x = self.hw(l_n, l_nu, x_u, l_u, a, b)\n",
    "            node_states = self.aggr(x, edge_list, num_nodes)\n",
    "            x_u = torch.index_select(node_states, 0, edge_list[1])\n",
    "        logits = self.output_layer(node_states[0])\n",
    "        # logits = self.output_layer(torch.sum(node_states, dim=0))\n",
    "        loss = self.criterion(logits, label.float())\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:55, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m data_test \u001b[38;5;241m=\u001b[39m [data[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m test_index]\n\u001b[1;32m     13\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m MyDataset(data_train, device)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m MyDataset(data_test, device)\n\u001b[1;32m     16\u001b[0m acc, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, test_dataset)\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataset, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m4\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     26\u001b[0m }\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 252\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, num_nodes, node_labels, edge_list, edge_attr, label)\u001b[0m\n\u001b[1;32m    250\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrou(l_n \u001b[38;5;241m+\u001b[39m l_nu \u001b[38;5;241m+\u001b[39m l_u \u001b[38;5;241m+\u001b[39m x_u)\n\u001b[1;32m    251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhw(l_n, l_nu, x_u, l_u, a, b)\n\u001b[0;32m--> 252\u001b[0m     node_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     x_u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mindex_select(node_states, \u001b[38;5;241m0\u001b[39m, edge_list[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    254\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(node_states[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 187\u001b[0m, in \u001b[0;36mAggr.forward\u001b[0;34m(self, x, edge_list, num_node)\u001b[0m\n\u001b[1;32m    185\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(x)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_node):\n\u001b[0;32m--> 187\u001b[0m     x[i] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_u\u001b[49m\u001b[43m[\u001b[49m\u001b[43medge_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=100)\n",
    "acc_scores = []\n",
    "for train_index, test_index in tqdm(kf.split(data)):\n",
    "    model = GNN(embed_dim=100, t=30, **meta_data)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "    )\n",
    "    data_train = [data[i] for i in train_index]\n",
    "    data_test = [data[i] for i in test_index]\n",
    "    train_dataset = MyDataset(data_train, device)\n",
    "    train(model, optimizer, train_dataset, epochs=10)\n",
    "    test_dataset = MyDataset(data_test, device)\n",
    "    acc, _, _ = eval(model, test_dataset)\n",
    "    print(acc)\n",
    "    acc_scores.append(acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8111111164093018"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(acc_scores).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
