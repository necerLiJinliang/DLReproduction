{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用批量训练的方式进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data, Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>element</th>\n",
       "      <th>atom_type</th>\n",
       "      <th>charge</th>\n",
       "      <th>element_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>h</td>\n",
       "      <td>3</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  drug_id element  atom_type  charge  element_type\n",
       "0   0        0       c         22   -0.11             0\n",
       "1   1        0       c         22   -0.11             0\n",
       "2   2        0       c         22   -0.11             0\n",
       "3   3        0       c         22   -0.11             0\n",
       "4   4        0       h          3    0.15             1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom = pd.read_csv(\n",
    "    \"dataset/Mutagenesis-42/atoms.csv\",\n",
    "    delimiter=\";\",\n",
    ")\n",
    "bond = pd.read_csv(\"dataset/Mutagenesis-42/bonds.csv\", delimiter=\";\")\n",
    "molecule = pd.read_csv(\"dataset/Mutagenesis-42/drugs.csv\", delimiter=\";\")\n",
    "atom.head()\n",
    "elements = atom[\"element\"].unique()\n",
    "element2id = {element: i for i, element in enumerate(elements)}\n",
    "atom[\"element_type\"] = atom[\"element\"].map(element2id)\n",
    "atom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom[\"atom_type\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 0, 'h': 1, 'n': 2, 'o': 3, 'cl': 4, 'f': 5, 's': 6}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ind1</th>\n",
       "      <th>inda</th>\n",
       "      <th>act</th>\n",
       "      <th>logp</th>\n",
       "      <th>lumo</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>2.29</td>\n",
       "      <td>-3.025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4.35</td>\n",
       "      <td>-2.138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-1.429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-1.478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ind1  inda   act  logp   lumo  active\n",
       "0   0   0.0   0.0 -0.70  2.29 -3.025       0\n",
       "1   1   0.0   0.0  0.57  2.13 -0.798       1\n",
       "2   2   1.0   0.0  0.77  4.35 -2.138       1\n",
       "3   3   1.0   0.0 -0.22  5.41 -1.429       0\n",
       "4   4   1.0   0.0 -0.22  5.41 -1.478       0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>atom1_id</th>\n",
       "      <th>atom2_id</th>\n",
       "      <th>bond_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  drug_id  atom1_id  atom2_id  bond_type\n",
       "0   0        0         0        11          7\n",
       "1   1        0        11        19          7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19\n",
       "6    23\n",
       "Name: atom2_id, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond[\"atom2_id\"].loc[bond[\"atom1_id\"] == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_ids = list(range(molecule.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_dict = bond.values.tolist()\n",
    "edges_dict = {(s[2], s[3]): s[4] for s in edges_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_data():\n",
    "    data = []\n",
    "    for sample_id in tqdm(molecule_ids):\n",
    "        # 首先获取节点信息\n",
    "        nodes = atom[\"id\"].loc[atom[\"drug_id\"] == sample_id].tolist()\n",
    "        nodes = sorted(nodes)\n",
    "\n",
    "        # 构建edge_index 和 edge_attr\n",
    "        edge_attr = bond[\"bond_type\"].loc[bond[\"drug_id\"] == sample_id].tolist()\n",
    "        edge_attr = edge_attr + edge_attr\n",
    "        edge_attr = (\n",
    "            torch.LongTensor(\n",
    "                edge_attr,\n",
    "            ).unsqueeze(1)\n",
    "            - 1\n",
    "        )\n",
    "        edge_attr = torch.zeros([len(edge_attr), 7]).scatter_(1, edge_attr, 1)\n",
    "        source_nodes = bond[\"atom1_id\"].loc[bond[\"drug_id\"] == sample_id] - nodes[0]\n",
    "        target_nodes = bond[\"atom2_id\"].loc[bond[\"drug_id\"] == sample_id] - nodes[0]\n",
    "        source_nodes = source_nodes.tolist()\n",
    "        target_nodes = target_nodes.tolist()\n",
    "        edge_index = [\n",
    "            source_nodes + target_nodes,\n",
    "            target_nodes + source_nodes,\n",
    "        ]\n",
    "\n",
    "        # 获取节点的特征\n",
    "        ind1 = molecule[\"ind1\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        inda = molecule[\"inda\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        logp = molecule[\"logp\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        lumo = molecule[\"lumo\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        node_features = []\n",
    "        for node in nodes:\n",
    "            node_type_index = atom[\"element_type\"].loc[atom[\"id\"] == node].tolist()[0]\n",
    "            node_type = [0] * 7\n",
    "            node_type[node_type_index] = 1\n",
    "            node_charge = atom[\"charge\"].loc[atom[\"id\"] == node].tolist()[0]\n",
    "            node_feature = node_type + [node_charge, ind1, inda, logp, lumo]\n",
    "            node_features.append(node_feature)\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        sample = Data(\n",
    "            x=x,\n",
    "            edge_index=torch.LongTensor(edge_index),\n",
    "            edge_attr=edge_attr,\n",
    "            y=torch.LongTensor(\n",
    "                [molecule[\"active\"].loc[molecule[\"id\"] == sample_id].tolist()[0]]\n",
    "            ),\n",
    "        )\n",
    "        data.append(sample)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 222.82it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = get_molecule_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[26, 12], edge_index=[2, 54], edge_attr=[54, 7], y=[1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutagDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(MutagDataset, self).__init__()\n",
    "        self.data = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        data_list = [data for data in batch]\n",
    "        batch_data = Batch.from_data_list(data_list)\n",
    "        return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, optimizer, batch_size, train_dataset, test_dataset, epochs, verbose=True\n",
    "):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=MutagDataset.collate_fn,\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=1,\n",
    "        end_factor=0.01,\n",
    "        total_iters=epochs * len(train_dataloader),\n",
    "    )\n",
    "    model.train()\n",
    "    max_acc = -1\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = {\n",
    "                \"x\": batch.x.to(device),\n",
    "                \"edge_index\": batch.edge_index.to(device),\n",
    "                \"edge_attr\": batch.edge_attr.to(device),\n",
    "                \"label\": batch.y.to(device),\n",
    "                \"ptr\": batch.ptr.to(device),\n",
    "            }\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(**inputs)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        acc = eval(model, test_dataset)[0]\n",
    "        if verbose:\n",
    "            print(f\"Epoch:{epoch+1}, Loss:{total_loss}, Accuracy:{acc}\")\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "    return max_acc\n",
    "\n",
    "\n",
    "def eval(model, test_dataset):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=MutagDataset.collate_fn,\n",
    "    )\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for batch in test_loader:\n",
    "        inputs = {\n",
    "            \"x\": batch.x.to(device),\n",
    "            \"edge_index\": batch.edge_index.to(device),\n",
    "            \"edge_attr\": batch.edge_attr.to(device),\n",
    "            \"label\": batch.y.to(device),\n",
    "            \"ptr\": batch.ptr.to(device),\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(**inputs)\n",
    "        labels.append(inputs[\"label\"])\n",
    "        preds.append(torch.sigmoid(logits))\n",
    "    labels = torch.stack(labels)\n",
    "    preds = torch.stack(preds)\n",
    "    labels = labels.view(-1)\n",
    "    preds = preds.view(-1)\n",
    "    # acc = acc_cal(preds, labels)\n",
    "    # labels[labels==-1] = 0\n",
    "    preds = preds > 0.5\n",
    "    acc = (preds == labels).sum() / len(labels)\n",
    "    return acc, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[26, 12], edge_index=[2, 54], edge_attr=[54, 7], y=[1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 12,\n",
    "        embed_dim: int = 50,\n",
    "    ):\n",
    "        super(NodeEncoder, self).__init__()\n",
    "        self.node_embedding = nn.Linear(input_dim, embed_dim)\n",
    "        nn.init.xavier_uniform_(self.node_embedding.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.node_embedding(x)\n",
    "        return x  # [n,e]\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class EdgeEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 7,\n",
    "        embed_dim: int = 50,\n",
    "    ):\n",
    "        super(EdgeEncoder, self).__init__()\n",
    "        self.edge_embedding = nn.Linear(input_dim, embed_dim)\n",
    "        nn.init.xavier_uniform_(self.edge_embedding.weight)\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        edge_features = self.edge_embedding(edge_attr)  # [n,n,e]\n",
    "        return edge_features\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class HwNonLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        \"\"\"非线性的GNN的H_w函数实现，在利用节点、边、邻居节点的特征构造相关向量，\n",
    "        然后拼接/加和节点状态向量，使用三层的FNN网络计算新的节点状态。\n",
    "        Args:\n",
    "            num_atom_type (int): 原子的种类数量\n",
    "            num_ind1_type (int): ind1的种类数量\n",
    "            num_inda_type (int): inda的种类数量\n",
    "            embed_dim (int): 嵌入的维度\n",
    "        \"\"\"\n",
    "        super(HwNonLinear, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_features,\n",
    "        edge_features,\n",
    "        neighbor_state,\n",
    "        neighbor_features,\n",
    "    ):\n",
    "        \"\"\"实现h_w函数，节点的特征l_n、边的特征l_nu、邻居节点的状态x_u、邻居节点的特征l_u，结合前馈神经网络进行前项传播。\n",
    "        Args:\n",
    "            node_features (torch.Tensor): l_n [n,e]\n",
    "            edge_features (torch.Tensor): l_nu [n,n,e]\n",
    "            neighbor_state (torch.Tensor): x_u [n,e]\n",
    "            neighbor_features (torch.Tensor): l_u [n,e]\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # x = torch.cat(\n",
    "        #     [node_features, edge_features, neighbor_state, neighbor_features], dim=-1\n",
    "        # )\n",
    "        # x = self.mlp(x)\n",
    "\n",
    "        x = self.mlp2(\n",
    "            (node_features + edge_features + neighbor_state + neighbor_features) / 4\n",
    "        )\n",
    "        return x\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class Aggr(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super(Aggr, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        aggregate_map,\n",
    "    ):\n",
    "        \"\"\"聚合函数，聚合邻居节点的状态向量，得到新的节点状态向量。\n",
    "        Args:\n",
    "            x (torch.Tensor): 需要传递的信息向量 [m,e]\n",
    "            aggregate_map (torch.Tensor): 聚合映射矩阵 [m,n]\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        x = torch.einsum(\"me,mn->ne\", x, aggregate_map)\n",
    "        # x = x / aggregate_map.T.sum(dim=-1, keepdim=True)\n",
    "        # x = x / aggregate_map.T.norm(dim=-1, keepdim=True,p=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        node_input_dim=12,\n",
    "        edge_input_dim=7,\n",
    "        t=10,\n",
    "    ):\n",
    "        super(GNN, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.t = t\n",
    "        self.node_encoder = NodeEncoder(node_input_dim, embed_dim)\n",
    "        self.edge_encoder = EdgeEncoder(edge_input_dim, embed_dim)\n",
    "        self.hw = HwNonLinear(embed_dim)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, 1),\n",
    "        )\n",
    "        self.aggr = Aggr(embed_dim)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.l2_reg = nn.MSELoss()\n",
    "        self.thresh = 1e-5\n",
    "        nn.init.xavier_uniform_(self.output_layer[0].weight)\n",
    "\n",
    "    def contraction_penalty(self, params, threshold=0.9):\n",
    "        \"\"\"计算收缩映射的惩罚项\"\"\"\n",
    "        penalty = 0\n",
    "        for param in params:\n",
    "            # 计算参数的范数\n",
    "            norm = torch.norm(param, p=2)\n",
    "            # 如果范数大于阈值，则添加惩罚项\n",
    "            penalty += torch.pow(torch.relu(norm - threshold), 2)\n",
    "        return penalty\n",
    "\n",
    "    def get_aggregate_map(self, edge_index, num_nodes):\n",
    "        \"\"\"获取聚合映射矩阵，将邻接表中的第二个位置的节点的信息聚合到第一个位置的节点上。\n",
    "\n",
    "        Args:\n",
    "            edge_index (torch.Tensor): [2,m]\n",
    "            num_nodes (int): 节点的数量\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [m,n]\n",
    "        \"\"\"\n",
    "        aggregate_map = torch.zeros(edge_index.shape[1], num_nodes)\n",
    "        aggregate_map[range(aggregate_map.shape[0]), edge_index[0]] = 1\n",
    "        return aggregate_map\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, label, ptr):\n",
    "        \"\"\"前向传播\n",
    "        Args:\n",
    "            num_nodes (int): 节点的数量\n",
    "            node_labels (torch.Tensor): 节点的标签 [n,e]\n",
    "            edge_index (torch.Tensor): 边的列表 [2,n]\n",
    "            edge_attr (torch.Tensor): 边的属性 [n,n]\n",
    "            label (torch.Tensor): 标签 [1]\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        num_nodes = x.shape[0]\n",
    "        if self.training:\n",
    "            node_states = torch.zeros(\n",
    "                num_nodes, self.embed_dim, requires_grad=False\n",
    "            ).to(x)\n",
    "        else:\n",
    "            node_states = torch.zeros(\n",
    "                num_nodes, self.embed_dim, requires_grad=False\n",
    "            ).to(\n",
    "                x\n",
    "            )  # [n,e]\n",
    "        aggregate_map = self.get_aggregate_map(edge_index, num_nodes).to(x)\n",
    "        node_features = self.node_encoder(x)  # [n,e]\n",
    "        edge_features = self.edge_encoder(edge_attr)  # [m,e]\n",
    "        # node_states = node_features\n",
    "        l_n = torch.index_select(node_features, 0, edge_index[0])  # [m,e]\n",
    "        l_u = torch.index_select(node_features, 0, edge_index[1])\n",
    "        l_nu = edge_features\n",
    "        x_u = torch.index_select(node_states, 0, edge_index[1])\n",
    "        for i in range(self.t):\n",
    "            x = self.hw(l_n, l_nu, x_u, l_u)\n",
    "            new_state = self.aggr(x, aggregate_map)\n",
    "            # new_state = new_state / torch.norm(new_state, p=2, dim=-1, keepdim=True)\n",
    "            with torch.no_grad():\n",
    "                distance = torch.norm(new_state - node_states, p=2, dim=-1)\n",
    "                # print(distance.mean().item())\n",
    "                # print(new_state.mean().item())\n",
    "                check = distance < self.thresh\n",
    "            if check.all():\n",
    "                # print(\"yes\")\n",
    "                break\n",
    "            node_states = new_state\n",
    "            # node_states = new_state * 0.9 + node_states * 1\n",
    "            x_u = torch.index_select(node_states, 0, edge_index[1])\n",
    "        # logits = self.output_layer(torch.index_select(node_states,dim=0,index=ptr))  # [b,1]\n",
    "        logits = self.output_layer(\n",
    "            torch.index_select(node_states, dim=0, index=ptr[:-1])\n",
    "        )  # [b,1]\n",
    "        # logits = self.output_layer(node_states.mean(dim=0))\n",
    "        hw_params = self.hw.get_parameters()\n",
    "        node_encoder_params = self.node_encoder.get_parameters()\n",
    "        edge_encoder_params = self.edge_encoder.get_parameters()\n",
    "        penalty = (\n",
    "            self.contraction_penalty(hw_params, threshold=1)\n",
    "            + self.contraction_penalty(node_encoder_params, threshold=1)\n",
    "            + self.contraction_penalty(edge_encoder_params, threshold=1)\n",
    "        )\n",
    "        label[label == -1] = 0\n",
    "        # print(label)\n",
    "        loss = self.criterion(logits.view(-1), label.float())\n",
    "        # node_states.requires_grad_(True)\n",
    "        # grad_x = torch.autograd.grad(loss, node_states, create_graph=True)[0]\n",
    "        return logits, loss + 0.01 * penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:39.95088902115822, Accuracy:0.800000011920929\n",
      "Epoch:2, Loss:36.88047909736633, Accuracy:0.800000011920929\n",
      "Epoch:3, Loss:32.263562858104706, Accuracy:1.0\n",
      "Epoch:4, Loss:26.761616557836533, Accuracy:1.0\n",
      "Epoch:5, Loss:25.750604957342148, Accuracy:1.0\n",
      "Epoch:6, Loss:25.568319715559483, Accuracy:1.0\n",
      "Epoch:7, Loss:24.370466731488705, Accuracy:1.0\n",
      "Epoch:8, Loss:25.288087122142315, Accuracy:1.0\n",
      "Epoch:9, Loss:23.510294195264578, Accuracy:1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, batch_size, train_dataset, test_dataset, epochs, verbose)\u001b[0m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.1, random_state=3)\n",
    "train_dataset = MutagDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataset = MutagDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "model = GNN(embed_dim=50, t=1000, edge_input_dim=7, node_input_dim=12)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "acc = train(model, optimizer, batch_size, train_dataset, test_dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8000)\n",
      "tensor(1.)\n",
      "tensor(0.7500)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(0.7500)\n",
      "tensor(0.7500)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m GNN(embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, edge_input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, node_input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m     13\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m all_acc\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc)\n",
      "Cell \u001b[0;32mIn[20], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, batch_size, train_dataset, test_dataset, epochs, verbose)\u001b[0m\n\u001b[1;32m     29\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:20\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:50\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@_no_grad\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_grad_norm_\u001b[39m(\n\u001b[1;32m     26\u001b[0m         parameters: _tensor_or_tensors, max_norm: \u001b[38;5;28mfloat\u001b[39m, norm_type: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m,\n\u001b[1;32m     27\u001b[0m         error_if_nonfinite: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, foreach: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Clip the gradient norm of an iterable of parameters.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    The norm is computed over all gradients together, as if they were\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m        Total norm of the parameter gradients (viewed as a single vector).\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     51\u001b[0m         parameters \u001b[38;5;241m=\u001b[39m [parameters]\n\u001b[1;32m     52\u001b[0m     grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "batch_size = 1\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "kf.split(dataset)\n",
    "all_acc = []\n",
    "for train_index, test_index in kf.split(dataset):\n",
    "    train_data = [dataset[i] for i in train_index]\n",
    "    test_data = [dataset[i] for i in test_index]\n",
    "    train_dataset = MutagDataset(train_data)\n",
    "    test_dataset = MutagDataset(test_data)\n",
    "    model = GNN(embed_dim=50, t=1000, edge_input_dim=7, node_input_dim=12)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    acc = train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        batch_size,\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        epochs=100,\n",
    "        verbose=False,\n",
    "    )\n",
    "    all_acc.append(acc)\n",
    "    print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
