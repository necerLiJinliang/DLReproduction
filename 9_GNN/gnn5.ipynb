{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data, Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root=\"./dataset/temp\", name=\"MUTAG\")\n",
    "# data = dataset.shuffle()\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.1, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.datasets.tu_dataset.TUDataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 17, 30, 43, 62])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = Batch.from_data_list(dataset[0:4])\n",
    "batch.ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutagDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super(MutagDataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        return Batch.from_data_list(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataset, test_dataset, epochs, verbose=True):\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=MutagDataset.collate_fn,\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=1,\n",
    "        end_factor=0.01,\n",
    "        total_iters=epochs * len(train_dataloader),\n",
    "    )\n",
    "    model.train()\n",
    "    max_acc = -1\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = {\n",
    "                \"x\": batch.x.to(device),\n",
    "                \"edge_index\": batch.edge_index.to(device),\n",
    "                \"edge_attr\": batch.edge_attr.to(device),\n",
    "                \"label\": batch.y.to(device),\n",
    "                \"ptr\": batch.ptr.to(device),\n",
    "            }\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(**inputs)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        acc = eval(model, test_dataset)[0]\n",
    "        if verbose:\n",
    "            print(f\"Epoch:{epoch+1}, Loss:{total_loss}, Accuracy:{acc}\")\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "    return max_acc\n",
    "\n",
    "\n",
    "def eval(model, test_dataset):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=MutagDataset.collate_fn,\n",
    "    )\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for batch in test_loader:\n",
    "        inputs = {\n",
    "            \"x\": batch.x.to(device),\n",
    "            \"edge_index\": batch.edge_index.to(device),\n",
    "            \"edge_attr\": batch.edge_attr.to(device),\n",
    "            \"label\": batch.y.to(device),\n",
    "            \"ptr\": batch.ptr.to(device),\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(**inputs)\n",
    "        labels.append(inputs[\"label\"])\n",
    "        preds.append(torch.sigmoid(logits))\n",
    "    labels = torch.stack(labels)\n",
    "    preds = torch.stack(preds)\n",
    "    labels = labels.view(-1)\n",
    "    preds = preds.view(-1)\n",
    "    # acc = acc_cal(preds, labels)\n",
    "    # labels[labels==-1] = 0\n",
    "    preds = preds > 0.5\n",
    "    acc = (preds == labels).sum() / len(labels)\n",
    "    return acc, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super(NodeEncoder, self).__init__()\n",
    "        self.node_embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.node_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.mlp[0].weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.node_embedding(x)\n",
    "        # x = torch.tanh(x)\n",
    "        # x = x / torch.norm(x, p=2, dim=-1, keepdim=True)\n",
    "        return x  # [n,e]\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class EdgeEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super(EdgeEncoder, self).__init__()\n",
    "        self.edge_embedding = nn.Linear(input_dim, embed_dim)\n",
    "        nn.init.xavier_uniform_(self.edge_embedding.weight)\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        edge_features = self.edge_embedding(edge_attr)  # [n,n,e]\n",
    "        # edge_features = torch.tanh(edge_features)\n",
    "        # edge_features = edge_features / torch.norm(\n",
    "        #     edge_features, p=2, dim=-1, keepdim=True\n",
    "        # )\n",
    "        return edge_features\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class HwNonLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        \"\"\"非线性的GNN的H_w函数实现，在利用节点、边、邻居节点的特征构造相关向量，\n",
    "        然后拼接/加和节点状态向量，使用三层的FNN网络计算新的节点状态。\n",
    "        Args:\n",
    "            num_atom_type (int): 原子的种类数量\n",
    "            num_ind1_type (int): ind1的种类数量\n",
    "            num_inda_type (int): inda的种类数量\n",
    "            embed_dim (int): 嵌入的维度\n",
    "        \"\"\"\n",
    "        super(HwNonLinear, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Linear(embed_dim, embed_dim),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        # nn.init.xavier_uniform_(self.mlp[0].weight)\n",
    "        # nn.init.xavier_uniform_(self.mlp[2].weight)\n",
    "        # nn.init.xavier_uniform_(self.mlp2[0].weight)\n",
    "        # nn.init.xavier_uniform_(self.mlp2[2].weight)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_features,\n",
    "        edge_features,\n",
    "        neighbor_state,\n",
    "        neighbor_features,\n",
    "    ):\n",
    "        \"\"\"实现h_w函数，节点的特征l_n、边的特征l_nu、邻居节点的状态x_u、邻居节点的特征l_u，结合前馈神经网络进行前项传播。\n",
    "        Args:\n",
    "            node_features (torch.Tensor): l_n [n,e]\n",
    "            edge_features (torch.Tensor): l_nu [n,n,e]\n",
    "            neighbor_state (torch.Tensor): x_u [n,e]\n",
    "            neighbor_features (torch.Tensor): l_u [n,e]\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # x = torch.cat(\n",
    "        #     [node_features, edge_features, neighbor_state, neighbor_features], dim=-1\n",
    "        # )\n",
    "        # x = self.mlp(x)\n",
    "\n",
    "        x = self.mlp2((edge_features + neighbor_state + neighbor_features) / 3)\n",
    "        return x\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class Aggr(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super(Aggr, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        aggregate_map,\n",
    "    ):\n",
    "        \"\"\"聚合函数，聚合邻居节点的状态向量，得到新的节点状态向量。\n",
    "        Args:\n",
    "            x (torch.Tensor): 需要传递的信息向量 [m,e]\n",
    "            aggregate_map (torch.Tensor): 聚合映射矩阵 [m,n]\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        x = torch.einsum(\"me,mn->ne\", x, aggregate_map)\n",
    "        # x = x / aggregate_map.T.sum(dim=-1, keepdim=True)\n",
    "        # x = x / aggregate_map.T.norm(dim=-1, keepdim=True,p=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        node_input_dim=7,\n",
    "        edge_input_dim=4,\n",
    "        t=10,\n",
    "    ):\n",
    "        super(GNN, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.t = t\n",
    "        self.node_encoder = NodeEncoder(node_input_dim, embed_dim)\n",
    "        self.edge_encoder = EdgeEncoder(edge_input_dim, embed_dim)\n",
    "        self.hw = HwNonLinear(embed_dim)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, 1),\n",
    "        )\n",
    "        self.aggr = Aggr(embed_dim)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.l2_reg = nn.MSELoss()\n",
    "        self.thresh = 1e-5\n",
    "        nn.init.xavier_uniform_(self.output_layer[0].weight)\n",
    "\n",
    "    def contraction_penalty(self, params, threshold=0.9):\n",
    "        \"\"\"计算收缩映射的惩罚项\"\"\"\n",
    "        penalty = 0\n",
    "        for param in params:\n",
    "            # 计算参数的范数\n",
    "            norm = torch.norm(param, p=2)\n",
    "            # 如果范数大于阈值，则添加惩罚项\n",
    "            penalty += torch.pow(torch.relu(norm - threshold), 2)\n",
    "        return penalty\n",
    "\n",
    "    def get_aggregate_map(self, edge_index, num_nodes):\n",
    "        \"\"\"获取聚合映射矩阵，将邻接表中的第二个位置的节点的信息聚合到第一个位置的节点上。\n",
    "\n",
    "        Args:\n",
    "            edge_index (torch.Tensor): [2,m]\n",
    "            num_nodes (int): 节点的数量\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [m,n]\n",
    "        \"\"\"\n",
    "        aggregate_map = torch.zeros(edge_index.shape[1], num_nodes)\n",
    "        aggregate_map[range(aggregate_map.shape[0]), edge_index[0]] = 1\n",
    "        return aggregate_map\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, label, ptr):\n",
    "        \"\"\"前向传播\n",
    "        Args:\n",
    "            num_nodes (int): 节点的数量\n",
    "            node_labels (torch.Tensor): 节点的标签 [n,e]\n",
    "            edge_index (torch.Tensor): 边的列表 [2,n]\n",
    "            edge_attr (torch.Tensor): 边的属性 [n,n]\n",
    "            label (torch.Tensor): 标签 [1]\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        num_nodes = x.shape[0]\n",
    "        if self.training:\n",
    "            node_states = torch.zeros(\n",
    "                num_nodes, self.embed_dim, requires_grad=False\n",
    "            ).to(x)\n",
    "        else:\n",
    "            node_states = torch.zeros(\n",
    "                num_nodes, self.embed_dim, requires_grad=False\n",
    "            ).to(\n",
    "                x\n",
    "            )  # [n,e]\n",
    "        aggregate_map = self.get_aggregate_map(edge_index, num_nodes).to(x)\n",
    "        node_features = self.node_encoder(x)  # [n,e]\n",
    "        edge_features = self.edge_encoder(edge_attr)  # [m,e]\n",
    "        # node_states = node_features\n",
    "        l_n = torch.index_select(node_features, 0, edge_index[0])  # [m,e]\n",
    "        l_u = torch.index_select(node_features, 0, edge_index[1])\n",
    "        l_nu = edge_features\n",
    "        x_u = torch.index_select(node_states, 0, edge_index[1])\n",
    "        for i in range(self.t):\n",
    "            x = self.hw(l_n, l_nu, x_u, l_u)\n",
    "            new_state = self.aggr(x, aggregate_map)\n",
    "            # new_state = new_state / torch.norm(new_state, p=2, dim=-1, keepdim=True)\n",
    "            with torch.no_grad():\n",
    "                distance = torch.norm(new_state - node_states, p=2, dim=-1)\n",
    "                # print(distance.mean().item())\n",
    "                # print(new_state.mean().item())\n",
    "                check = distance < self.thresh\n",
    "            if check.all():\n",
    "                # print(\"yes\")\n",
    "                break\n",
    "            node_states = new_state\n",
    "            # node_states = new_state * 0.9 + node_states * 0.1\n",
    "            x_u = torch.index_select(node_states, 0, edge_index[1])\n",
    "        # logits = self.output_layer(torch.index_select(node_states,dim=0,index=ptr))  # [b,1]\n",
    "        logits = self.output_layer(\n",
    "            torch.index_select(node_states, dim=0, index=ptr[:-1])\n",
    "        )  # [b,1]\n",
    "        # logits = self.output_layer(node_states.mean(dim=0))\n",
    "        hw_params = self.hw.get_parameters()\n",
    "        node_encoder_params = self.node_encoder.get_parameters()\n",
    "        edge_encoder_params = self.edge_encoder.get_parameters()\n",
    "        penalty = (\n",
    "            self.contraction_penalty(hw_params, threshold=0.1)\n",
    "            + self.contraction_penalty(node_encoder_params, threshold=0.1)\n",
    "            + self.contraction_penalty(edge_encoder_params, threshold=0.1)\n",
    "        )\n",
    "        label[label == -1] = 0\n",
    "        loss = self.criterion(logits.view(-1), label.float())\n",
    "        # node_states.requires_grad_(True)\n",
    "        # grad_x = torch.autograd.grad(loss, node_states, create_graph=True)[0]\n",
    "        return logits, loss + 0.01 * penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:134.00599282979965, Accuracy:0.5263158082962036\n",
      "Epoch:2, Loss:121.7857601493597, Accuracy:0.5263158082962036\n",
      "Epoch:3, Loss:137.7623077481985, Accuracy:0.5263158082962036\n",
      "Epoch:4, Loss:145.52199064195156, Accuracy:0.5263158082962036\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 33\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataset, test_dataset, epochs, verbose)\u001b[0m\n\u001b[1;32m     31\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     32\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 33\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 60\u001b[0m, in \u001b[0;36meval\u001b[0;34m(model, test_dataset)\u001b[0m\n\u001b[1;32m     52\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mptr\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch\u001b[38;5;241m.\u001b[39mptr\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     58\u001b[0m }\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 60\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     62\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39msigmoid(logits))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[41], line 224\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, x, edge_index, edge_attr, label, ptr)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# new_state = new_state / torch.norm(new_state, p=2, dim=-1, keepdim=True)\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 224\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnode_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# print(distance.mean().item())\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# print(new_state.mean().item())\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     check \u001b[38;5;241m=\u001b[39m distance \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthresh\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/functional.py:1631\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1629\u001b[0m _p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m p\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\u001b[38;5;28minput\u001b[39m, _p, _dim, keepdim, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root=\"./dataset/temp\", name=\"MUTAG\")\n",
    "# data = dataset.shuffle()\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.1, random_state=2)\n",
    "train_dataset = MutagDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataset = MutagDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "model = GNN(embed_dim=50, t=1000, edge_input_dim=4, node_input_dim=7)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "acc = train(model, optimizer, train_dataset, test_dataset, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.mps' from '/Users/lijinliang/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/mps/__init__.py'>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m GNN(embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, edge_input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, node_input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m all_acc\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc)\n",
      "Cell \u001b[0;32mIn[100], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataset, test_dataset, epochs, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m2\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[107], line 212\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, x, edge_index, edge_attr, label)\u001b[0m\n\u001b[1;32m    206\u001b[0m     node_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    207\u001b[0m         num_nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    209\u001b[0m         x\n\u001b[1;32m    210\u001b[0m     )  \u001b[38;5;66;03m# [n,e]\u001b[39;00m\n\u001b[1;32m    211\u001b[0m aggregate_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_aggregate_map(edge_index, num_nodes)\u001b[38;5;241m.\u001b[39mto(x)\n\u001b[0;32m--> 212\u001b[0m node_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [n,e]\u001b[39;00m\n\u001b[1;32m    213\u001b[0m edge_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_encoder(edge_attr)  \u001b[38;5;66;03m# [m,e]\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# node_states = node_features\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[107], line 18\u001b[0m, in \u001b[0;36mNodeEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# x = torch.tanh(x)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# x = x / torch.norm(x, p=2, dim=-1, keepdim=True)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "kf.split(dataset)\n",
    "all_acc = []\n",
    "for train_index, test_index in kf.split(dataset):\n",
    "    train_data = dataset[train_index]\n",
    "    test_data = dataset[test_index]\n",
    "    train_dataset = MutagDataset(train_data)\n",
    "    test_dataset = MutagDataset(test_data)\n",
    "    model = GNN(embed_dim=50, t=1000, edge_input_dim=4, node_input_dim=7)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    acc = train(\n",
    "        model, optimizer, train_dataset, test_dataset, epochs=100, verbose=False\n",
    "    )\n",
    "    all_acc.append(acc)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:12, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:24, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:36, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:48, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:01, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:13, 12.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:25, 12.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:37, 12.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:49, 12.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [02:02, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "acc_scores = []\n",
    "for train_index, test_index in tqdm(kf.split(data)):\n",
    "    model = GNN(embed_dim=100, t=30, **meta_data)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "    )\n",
    "    data_train = [data[i] for i in train_index]\n",
    "    data_test = [data[i] for i in test_index]\n",
    "    train_dataset = MyDataset(data_train, device)\n",
    "    train(model, optimizer, train_dataset, epochs=30)\n",
    "    test_dataset = MyDataset(data_test, device)\n",
    "    acc, _, _ = eval(model, test_dataset)\n",
    "    print(acc)\n",
    "    acc_scores.append(acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6600)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(acc_scores).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
