{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接使用标签作为向量，不嵌入了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>element</th>\n",
       "      <th>atom_type</th>\n",
       "      <th>charge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>h</td>\n",
       "      <td>3</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  drug_id element  atom_type  charge\n",
       "0   0        0       c         22   -0.11\n",
       "1   1        0       c         22   -0.11\n",
       "2   2        0       c         22   -0.11\n",
       "3   3        0       c         22   -0.11\n",
       "4   4        0       h          3    0.15"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom = pd.read_csv(\n",
    "    \"dataset/Mutagenesis-42/atoms.csv\",\n",
    "    delimiter=\";\",\n",
    ")\n",
    "bond = pd.read_csv(\"dataset/Mutagenesis-42/bonds.csv\", delimiter=\";\")\n",
    "molecule = pd.read_csv(\"dataset/Mutagenesis-42/drugs.csv\", delimiter=\";\")\n",
    "atom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ind1</th>\n",
       "      <th>inda</th>\n",
       "      <th>act</th>\n",
       "      <th>logp</th>\n",
       "      <th>lumo</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>2.29</td>\n",
       "      <td>-3.025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2.13</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4.35</td>\n",
       "      <td>-2.138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-1.429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-1.478</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ind1  inda   act  logp   lumo  active\n",
       "0   0   0.0   0.0 -0.70  2.29 -3.025       0\n",
       "1   1   0.0   0.0  0.57  2.13 -0.798       1\n",
       "2   2   1.0   0.0  0.77  4.35 -2.138       1\n",
       "3   3   1.0   0.0 -0.22  5.41 -1.429       0\n",
       "4   4   1.0   0.0 -0.22  5.41 -1.478       0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>drug_id</th>\n",
       "      <th>atom1_id</th>\n",
       "      <th>atom2_id</th>\n",
       "      <th>bond_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  drug_id  atom1_id  atom2_id  bond_type\n",
       "0    0        0         0        11          7\n",
       "1    1        0        11        19          7\n",
       "2    2        0        19        20          7\n",
       "3    3        0        20        21          7\n",
       "4    4        0        21        22          7\n",
       "5    5        0        22         0          7\n",
       "6    6        0        11        23          1\n",
       "7    7        0        22        24          1\n",
       "8    8        0        19        25          7\n",
       "9    9        0        25         1          7\n",
       "10  10        0         1         2          7\n",
       "11  11        0         2         3          7\n",
       "12  12        0         3        20          7\n",
       "13  13        0        25         4          1\n",
       "14  14        0         2         5          1\n",
       "15  15        0         3         6          1\n",
       "16  16        0         1         7          1\n",
       "17  17        0        21         8          1\n",
       "18  18        0         0         9          1\n",
       "19  19        0         8        10          2\n",
       "20  20        0         8        12          2\n",
       "21  21        0         6        13          2\n",
       "22  22        0         6        14          2\n",
       "23  23        0         7        15          2\n",
       "24  24        0         7        16          2\n",
       "25  25        0         9        17          2\n",
       "26  26        0         9        18          2\n",
       "27  27        1        26        36          7\n",
       "28  28        1        36        37          7\n",
       "29  29        1        37        38          7\n",
       "30  30        1        38        39          7\n",
       "31  31        1        39        40          7\n",
       "32  32        1        40        26          7\n",
       "33  33        1        37        41          7\n",
       "34  34        1        41        42          7\n",
       "35  35        1        38        43          7\n",
       "36  36        1        43        42          7\n",
       "37  37        1        40        27          1\n",
       "38  38        1        27        28          2\n",
       "39  39        1        27        29          2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19\n",
       "6    23\n",
       "Name: atom2_id, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond[\"atom2_id\"].loc[bond[\"atom1_id\"] == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule_ids = list(range(molecule.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_dict = bond.values.tolist()\n",
    "edges_dict = {(s[2], s[3]): s[4] for s in edges_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_data():\n",
    "    data = []\n",
    "    for sample_id in tqdm(molecule_ids):\n",
    "        # 首先获取节点信息\n",
    "        nodes = atom[\"id\"].loc[atom[\"drug_id\"] == sample_id].tolist()\n",
    "        nodes = sorted(nodes)\n",
    "\n",
    "        # 构建edge_list 和 edge_attr\n",
    "        edge_attr = bond[\"bond_type\"].loc[bond[\"drug_id\"] == sample_id].tolist()\n",
    "        edge_attr = edge_attr + edge_attr\n",
    "        source_nodes = bond[\"atom1_id\"].loc[bond[\"drug_id\"] == sample_id] - nodes[0]\n",
    "        target_nodes = bond[\"atom2_id\"].loc[bond[\"drug_id\"] == sample_id] - nodes[0]\n",
    "        source_nodes = source_nodes.tolist()\n",
    "        target_nodes = target_nodes.tolist()\n",
    "        self_connect = [i for i in range(len(nodes))]\n",
    "        edge_list = [\n",
    "            source_nodes + target_nodes,\n",
    "            target_nodes + source_nodes,\n",
    "        ]\n",
    "\n",
    "        # 获取前全局特征\n",
    "        ind1 = molecule[\"ind1\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        inda = molecule[\"inda\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        logp = molecule[\"logp\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        lumo = molecule[\"lumo\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        global_feature = {\"ind1\": ind1, \"inda\": inda, \"logp\": logp, \"lumo\": lumo}\n",
    "        # 获取每个节点的特征\n",
    "        node_features = []\n",
    "        mean_charge = 0\n",
    "        for node in nodes:\n",
    "            node_type = atom[\"atom_type\"].loc[atom[\"id\"] == node].tolist()[0]\n",
    "            node_charge = atom[\"charge\"].loc[atom[\"id\"] == node].tolist()[0]\n",
    "            mean_charge += node_charge\n",
    "            node_feature = {\"type\": node_type, \"charge\": node_charge}\n",
    "            node_features.append(node_feature)\n",
    "        sample = dict()\n",
    "        sample[\"drug_id\"] = sample_id\n",
    "        nodes = np.array(nodes) - nodes[0]\n",
    "        nodes = nodes.astype(np.int64).tolist()\n",
    "        # nodes.append(len(nodes) - 1)\n",
    "        sample[\"nodes\"] = nodes\n",
    "        sample[\"edge_list\"] = edge_list\n",
    "        sample[\"edge_attr\"] = edge_attr\n",
    "        sample[\"global_features\"] = global_feature\n",
    "        sample[\"node_features\"] = node_features\n",
    "        sample[\"label\"] = (\n",
    "            molecule[\"active\"].loc[molecule[\"id\"] == sample_id].tolist()[0]\n",
    "        )\n",
    "        data.append(sample)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 150.07it/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_molecule_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, device):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        nodes = sample[\"nodes\"]\n",
    "        edge_list = sample[\"edge_list\"]\n",
    "        label = sample[\"label\"]\n",
    "        atom_type = [t[\"type\"] for t in sample[\"node_features\"]]\n",
    "        charge = [t[\"charge\"] for t in sample[\"node_features\"]]\n",
    "        ind1 = [sample[\"global_features\"][\"ind1\"]] * len(nodes)\n",
    "        inda = [sample[\"global_features\"][\"inda\"]] * len(nodes)\n",
    "        logp = [sample[\"global_features\"][\"logp\"]] * len(nodes)\n",
    "        lumo = [sample[\"global_features\"][\"lumo\"]] * len(nodes)\n",
    "        l_n = {\n",
    "            \"atom_type\": torch.LongTensor(atom_type).to(self.device),\n",
    "            \"charge\": torch.tensor(charge).to(self.device),\n",
    "            \"ind1\": torch.LongTensor(ind1).to(self.device),\n",
    "            \"inda\": torch.LongTensor(inda).to(self.device),\n",
    "            \"logp\": torch.tensor(logp).to(self.device),\n",
    "            \"lumo\": torch.tensor(lumo).to(self.device),\n",
    "        }\n",
    "        edge_list = torch.LongTensor(edge_list).to(self.device)\n",
    "        edge_attr = torch.LongTensor(sample[\"edge_attr\"]).to(self.device)\n",
    "        label = torch.tensor([label]).to(self.device)\n",
    "        num_nodes = len(nodes)\n",
    "        return {\n",
    "            \"num_nodes\": num_nodes,\n",
    "            \"node_labels\": l_n,\n",
    "            \"edge_list\": edge_list,\n",
    "            \"edge_attr\": edge_attr,\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        num_nodes = [sample[\"num_nodes\"] for sample in batch]\n",
    "        node_labels = [sample[\"node_labels\"] for sample in batch]\n",
    "        edges_list = [sample[\"edge_list\"] for sample in batch]\n",
    "        label = [sample[\"label\"] for sample in batch]\n",
    "        edge_attr = [sample[\"edge_attr\"] for sample in batch]\n",
    "        return num_nodes, node_labels, edges_list, edge_attr, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataset, test_dataset, epochs):\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=MyDataset.collate_fn,\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        start_factor=1,\n",
    "        end_factor=0.1,\n",
    "        total_iters=epochs * len(train_dataloader),\n",
    "    )\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs, gamma=0.7)\n",
    "    max_acc = -1\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs = {\n",
    "                \"num_nodes\": batch[0][0],\n",
    "                \"node_labels\": batch[1][0],\n",
    "                \"edge_list\": batch[2][0],\n",
    "                \"edge_attr\": batch[3][0],\n",
    "                \"label\": batch[4][0],\n",
    "            }\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(**inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            scheduler.step()\n",
    "        acc = eval(model, test_dataset)[0]\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "    return max_acc\n",
    "\n",
    "    # print(f\"Epoch:{epoch+1}, Loss:{total_loss}\")\n",
    "\n",
    "\n",
    "def eval(model, test_dataset):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=1, shuffle=False, collate_fn=MyDataset.collate_fn\n",
    "    )\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for batch in test_loader:\n",
    "        inputs = {\n",
    "            \"num_nodes\": batch[0][0],\n",
    "            \"node_labels\": batch[1][0],\n",
    "            \"edge_list\": batch[2][0],\n",
    "            \"edge_attr\": batch[3][0],\n",
    "            \"label\": batch[4][0],\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(**inputs)\n",
    "        labels.append(inputs[\"label\"])\n",
    "        preds.append(torch.sigmoid(logits))\n",
    "    labels = torch.stack(labels)\n",
    "    preds = torch.stack(preds)\n",
    "    labels = labels.view(-1)\n",
    "    preds = preds.view(-1)\n",
    "    # acc = acc_cal(preds, labels)\n",
    "    # labels[labels==-1] = 0\n",
    "    preds = preds > 0.5\n",
    "    acc = (preds == labels).sum() / len(labels)\n",
    "    return acc, preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=100)\n",
    "test_dataset = MyDataset(test_data, device)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset = MyDataset(train_data, device)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=MyDataset.collate_fn,\n",
    ")\n",
    "meta_data = {\n",
    "    \"num_atom_type\": 233,\n",
    "    \"num_ind1_type\": 2,\n",
    "    \"num_inda_type\": 2,\n",
    "    \"num_edge_type\": 8,\n",
    "}\n",
    "# model = GNN(embed_dim=100, t=10, **meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        super(NodeEncoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, embed_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, embed_dim)\n",
    "        self.ind1_embedding = nn.Embedding(num_ind1_type, embed_dim)\n",
    "        self.inda_embedding = nn.Embedding(num_inda_type, embed_dim)\n",
    "        self.logp_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.lumo_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.charge_embedding = nn.Parameter(torch.randn(1, embed_dim))\n",
    "        self.trans = nn.Linear(embed_dim * 6, embed_dim)\n",
    "\n",
    "    def init(self):\n",
    "        nn.init.xavier_uniform_(self.atom_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.ind1_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.inda_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.logp_embedding)\n",
    "        nn.init.xavier_uniform_(self.lumo_embedding)\n",
    "        nn.init.xavier_uniform_(self.charge_embedding)\n",
    "\n",
    "    def forward(self, node_labels):\n",
    "        atom_embed = self.atom_embedding(node_labels[\"atom_type\"])  # [n,e]\n",
    "        ind1_embed = self.ind1_embedding(node_labels[\"ind1\"])\n",
    "        inda_embed = self.inda_embedding(node_labels[\"inda\"])\n",
    "        logp_embed = self.logp_embedding * node_labels[\"logp\"].unsqueeze(dim=-1)\n",
    "        lumo_embed = self.lumo_embedding * node_labels[\"lumo\"].unsqueeze(\n",
    "            dim=-1\n",
    "        )  # [n,e]\n",
    "        charge_embed = self.charge_embedding * node_labels[\"charge\"].unsqueeze(dim=-1)\n",
    "        # node_features = (\n",
    "        #     atom_embed\n",
    "        #     + ind1_embed\n",
    "        #     + inda_embed\n",
    "        #     + logp_embed\n",
    "        #     + lumo_embed\n",
    "        #     + charge_embed\n",
    "        # )\n",
    "        node_features = torch.cat(\n",
    "            [\n",
    "                atom_embed,\n",
    "                ind1_embed,\n",
    "                inda_embed,\n",
    "                logp_embed,\n",
    "                lumo_embed,\n",
    "                charge_embed,\n",
    "            ],\n",
    "            dim=-1,\n",
    "        )\n",
    "        node_features = torch.tanh(self.trans(node_features))\n",
    "        return node_features  # [n,e]\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class EdgeEncoder(nn.Module):\n",
    "    def __init__(self, num_edge_type: int, embed_dim: int):\n",
    "        super(EdgeEncoder, self).__init__()\n",
    "        self.edge_embedding = nn.Embedding(num_edge_type + 1, embed_dim)\n",
    "        nn.init.xavier_uniform_(self.edge_embedding.weight)\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        edge_features = self.edge_embedding(edge_attr)  # [n,n,e]\n",
    "        return edge_features\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class HwNonLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "    ):\n",
    "        \"\"\"非线性的GNN的H_w函数实现，在利用节点、边、邻居节点的特征构造相关向量，\n",
    "        然后拼接/加和节点状态向量，使用三层的FNN网络计算新的节点状态。\n",
    "        Args:\n",
    "            num_atom_type (int): 原子的种类数量\n",
    "            num_ind1_type (int): ind1的种类数量\n",
    "            num_inda_type (int): inda的种类数量\n",
    "            embed_dim (int): 嵌入的维度\n",
    "        \"\"\"\n",
    "        super(HwNonLinear, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.trans = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 4, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.trans[0].weight)  # 初始化参数\n",
    "        nn.init.xavier_uniform_(self.trans[2].weight)  # 初始化参数\n",
    "        # nn.init.xavier_uniform_(self.trans[4].weight)  # 初始化参数\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_features,\n",
    "        edge_features,\n",
    "        neighbor_state,\n",
    "        neighbor_features,\n",
    "    ):\n",
    "        \"\"\"实现h_w函数，节点的特征l_n、边的特征l_nu、邻居节点的状态x_u、邻居节点的特征l_u，结合前馈神经网络进行前项传播。\n",
    "        Args:\n",
    "            node_features (torch.Tensor): l_n [n,e]\n",
    "            edge_features (torch.Tensor): l_nu [n,n,e]\n",
    "            neighbor_state (torch.Tensor): x_u [n,e]\n",
    "            neighbor_features (torch.Tensor): l_u [n,e]\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # x = self.trans(neighbor_state)  # [m,e]\n",
    "        x = torch.cat(\n",
    "            [node_features, edge_features, neighbor_state, neighbor_features], dim=-1\n",
    "        )\n",
    "        x = self.trans(x)\n",
    "        return x\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"获取所有待学习参数\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "\n",
    "class Aggr(nn.Module):\n",
    "    def __init__(self, embed_dim: int):\n",
    "        super(Aggr, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        aggregate_map,\n",
    "    ):\n",
    "        \"\"\"聚合函数，聚合邻居节点的状态向量，得到新的节点状态向量。\n",
    "        Args:\n",
    "            x (torch.Tensor): 需要传递的信息向量 [m,e]\n",
    "            aggregate_map (torch.Tensor): 聚合映射矩阵 [m,n]\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        x = torch.einsum(\"me,mn->ne\", x, aggregate_map)\n",
    "        x = x / aggregate_map.T.sum(dim=-1, keepdim=True)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_atom_type: int,\n",
    "        num_ind1_type: int,\n",
    "        num_inda_type: int,\n",
    "        num_edge_type: int,\n",
    "        t,\n",
    "    ):\n",
    "        super(GNN, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.t = t\n",
    "        self.node_encoder = NodeEncoder(\n",
    "            num_atom_type, num_ind1_type, num_inda_type, embed_dim\n",
    "        )\n",
    "        self.edge_encoder = EdgeEncoder(num_edge_type, embed_dim)\n",
    "        self.hw = HwNonLinear(embed_dim)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim, 1),\n",
    "        )\n",
    "        self.aggr = Aggr(embed_dim)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        self.l2_reg = nn.MSELoss()\n",
    "        self.thresh = 1e-7\n",
    "\n",
    "    def contraction_penalty(self, params, threshold=0.9):\n",
    "        \"\"\"计算收缩映射的惩罚项\"\"\"\n",
    "        penalty = 0\n",
    "        for param in params:\n",
    "            # 计算参数的范数\n",
    "            norm = torch.norm(param, p=2)\n",
    "            # 如果范数大于阈值，则添加惩罚项\n",
    "            penalty += torch.pow(torch.relu(norm - threshold), 2)\n",
    "        return penalty\n",
    "\n",
    "    def get_aggregate_map(self, edge_list, num_nodes):\n",
    "        \"\"\"获取聚合映射矩阵，将邻接表中的第二个位置的节点的信息聚合到第一个位置的节点上。\n",
    "\n",
    "        Args:\n",
    "            edge_list (torch.Tensor): [2,m]\n",
    "            num_nodes (int): 节点的数量\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [m,n]\n",
    "        \"\"\"\n",
    "        aggregate_map = torch.zeros(edge_list.shape[1], num_nodes)\n",
    "        aggregate_map[range(aggregate_map.shape[0]), edge_list[0]] = 1\n",
    "        return aggregate_map\n",
    "\n",
    "    def forward(self, num_nodes, node_labels, edge_list, edge_attr, label):\n",
    "        \"\"\"前向传播\n",
    "        Args:\n",
    "            num_nodes (int): 节点的数量\n",
    "            node_labels (torch.Tensor): 节点的标签 [n,e]\n",
    "            edge_list (torch.Tensor): 边的列表 [2,n]\n",
    "            edge_attr (torch.Tensor): 边的属性 [n,n]\n",
    "            label (torch.Tensor): 标签 [1]\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            node_states = torch.zeros(num_nodes, self.embed_dim, requires_grad=False)\n",
    "        else:\n",
    "            node_states = torch.zeros(\n",
    "                num_nodes, self.embed_dim, requires_grad=False\n",
    "            )  # [n,e]\n",
    "        aggregate_map = self.get_aggregate_map(edge_list, num_nodes)\n",
    "        node_features = self.node_encoder(node_labels)  # [n,e]\n",
    "        node_states = node_features\n",
    "        edge_features = self.edge_encoder(edge_attr)  # [m,e]\n",
    "        l_n = torch.index_select(node_features, 0, edge_list[0])  # [m,e]\n",
    "        l_u = torch.index_select(node_features, 0, edge_list[1])\n",
    "        l_nu = edge_features\n",
    "        x_u = torch.index_select(node_states, 0, edge_list[1])\n",
    "        for i in range(self.t):\n",
    "            x = self.hw(l_n, l_nu, x_u, l_u)\n",
    "            new_state = self.aggr(x, aggregate_map)\n",
    "            with torch.no_grad():\n",
    "                distance = torch.norm(new_state - node_states, p=2, dim=-1)\n",
    "                # print(distance.mean())\n",
    "                check = distance < self.thresh\n",
    "            if check.all():\n",
    "                break\n",
    "            node_states = new_state\n",
    "            x_u = torch.index_select(node_states, 0, edge_list[1])\n",
    "        logits = self.output_layer(node_states[0])\n",
    "        # logits = self.output_layer(node_states.mean(dim=0))\n",
    "        hw_params = self.hw.get_parameters()\n",
    "        node_encoder_params = self.node_encoder.get_parameters()\n",
    "        edge_encoder_params = self.edge_encoder.get_parameters()\n",
    "        penalty = (\n",
    "            self.contraction_penalty(hw_params, threshold=1)\n",
    "            + self.contraction_penalty(node_encoder_params, threshold=1)\n",
    "            + self.contraction_penalty(edge_encoder_params, threshold=1)\n",
    "        )\n",
    "        loss = self.criterion(logits, label.float())\n",
    "        return logits, loss + penalty * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [04:41, 281.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [10:20, 315.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [13:58, 270.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [15:37, 312.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m MyDataset(data_train, device)\n\u001b[1;32m     14\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m MyDataset(data_test, device)\n\u001b[0;32m---> 15\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# acc, _, _ = eval(model, test_dataset)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc)\n",
      "Cell \u001b[0;32mIn[63], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataset, test_dataset, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;241m4\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     27\u001b[0m }\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 235\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, num_nodes, node_labels, edge_list, edge_attr, label)\u001b[0m\n\u001b[1;32m    233\u001b[0m x_u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mindex_select(node_states, \u001b[38;5;241m0\u001b[39m, edge_list[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt):\n\u001b[0;32m--> 235\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhw\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_nu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_u\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggr(x, aggregate_map)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 127\u001b[0m, in \u001b[0;36mHwNonLinear.forward\u001b[0;34m(self, node_features, edge_features, neighbor_state, neighbor_features)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# x = self.trans(neighbor_state)  # [m,e]\u001b[39;00m\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[1;32m    125\u001b[0m     [node_features, edge_features, neighbor_state, neighbor_features], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    126\u001b[0m )\n\u001b[0;32m--> 127\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "acc_scores = []\n",
    "for train_index, test_index in tqdm(kf.split(data)):\n",
    "    model = GNN(embed_dim=100, t=1000, **meta_data)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "    )\n",
    "    data_train = [data[i] for i in train_index]\n",
    "    data_test = [data[i] for i in test_index]\n",
    "    train_dataset = MyDataset(data_train, device)\n",
    "    test_dataset = MyDataset(data_test, device)\n",
    "    acc = train(model, optimizer, train_dataset, test_dataset, epochs=100)\n",
    "    # acc, _, _ = eval(model, test_dataset)\n",
    "    print(acc)\n",
    "    acc_scores.append(acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8750)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(acc_scores).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
