{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7abb44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Literal\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b455a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"k_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"down_proj\",\n",
    "        \"up_proj\",\n",
    "    ],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3194b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,495,680 || all params: 1,844,324,352 || trainable%: 0.4064\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../model_save/base_model/qwen-1.5-1.8b/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model_policy = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model_policy = prepare_model_for_kbit_training(model_policy)\n",
    "model_policy = get_peft_model(model_policy, peft_config)\n",
    "model_policy.print_trainable_parameters()\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./dataset/ultrafeedback/flan.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ba235",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d73594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_pad_batch_tensors(\n",
    "    tensors_list,\n",
    "    padding_value=0,\n",
    "    padding_side: Literal[\"left\", \"right\"] = \"left\",\n",
    "):\n",
    "    \"\"\"手动实现批次张量的填充\"\"\"\n",
    "    # 获取最大序列长度\n",
    "    max_seq_len = max(t.shape[1] for t in tensors_list)\n",
    "\n",
    "    # 创建填充后的张量列表\n",
    "    padded_tensors = []\n",
    "    for tensor in tensors_list:\n",
    "        b, s = tensor.shape\n",
    "        # 创建填充张量\n",
    "        padding = torch.full(\n",
    "            (b, max_seq_len - s),\n",
    "            padding_value,\n",
    "            dtype=tensor.dtype,\n",
    "            device=tensor.device,\n",
    "        )\n",
    "        # 拼接原始张量和填充部分\n",
    "        if padding_side == \"left\":\n",
    "            padded = torch.cat([padding, tensor], dim=1)\n",
    "        elif padding_side == \"right\":\n",
    "            # 如果是右侧填充，则将填充部分放在后面\n",
    "            padded = torch.cat([tensor, padding], dim=1)\n",
    "        padded_tensors.append(padded)\n",
    "\n",
    "    return torch.cat(padded_tensors, dim=0)\n",
    "\n",
    "\n",
    "def sample_trans(sample):\n",
    "    question = sample[\"instruction\"]\n",
    "    completions = sample[\"completions\"]\n",
    "    scores_list = [[0, 0], [0, 1], [0, 2], [0, 3]]\n",
    "    responses = []\n",
    "    for i, annotation in enumerate(completions):\n",
    "        responses.append(annotation[\"response\"])\n",
    "        scores_list[i][0] = float(annotation[\"overall_score\"])\n",
    "    scores_list = sorted(scores_list, key=lambda x: x[0], reverse=True)\n",
    "    results = []\n",
    "    for i in range(1, 4):\n",
    "        results.append(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"chosen\": responses[scores_list[0][1]],\n",
    "                \"rejected\": responses[scores_list[i][1]],\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_function(sample, tokenizer):\n",
    "    query = sample[\"question\"]\n",
    "    chosen = sample[\"chosen\"]\n",
    "    rejected = sample[\"rejected\"]\n",
    "    chosen_inputs = tokenizer(\n",
    "        \"\\n\".join([query, chosen]),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    query_len = len(tokenizer(query)[\"input_ids\"])\n",
    "    chosen_answer_len = chosen_inputs[\"input_ids\"][0].shape[0] - query_len\n",
    "    chosen_answer_len = max(chosen_answer_len, 0)\n",
    "    rejected_inputs = tokenizer(\n",
    "        \"\\n\".join([query, rejected]),\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "    rejected_answer_len = rejected_inputs[\"input_ids\"][0].shape[0] - query_len\n",
    "    rejected_answer_len = max(rejected_answer_len, 0)\n",
    "    return {\n",
    "        \"chosen\": chosen_inputs,\n",
    "        \"rejected\": rejected_inputs,\n",
    "        \"chosen_answer_len\": chosen_answer_len,\n",
    "        \"rejected_answer_len\": rejected_answer_len,\n",
    "    }\n",
    "\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    chosen_input_ids = [torch.LongTensor(s[\"chosen\"][\"input_ids\"][0]) for s in batch]\n",
    "    chosen_attention_mask = [\n",
    "        torch.tensor(s[\"chosen\"][\"attention_mask\"][0]) for s in batch\n",
    "    ]\n",
    "    rejected_input_ids = [\n",
    "        torch.LongTensor(s[\"rejected\"][\"input_ids\"][0]) for s in batch\n",
    "    ]\n",
    "    rejected_attention_mask = [\n",
    "        torch.tensor(s[\"rejected\"][\"attention_mask\"][0]) for s in batch\n",
    "    ]\n",
    "    chosen_inputs = tokenizer.pad(\n",
    "        {\n",
    "            \"input_ids\": chosen_input_ids,\n",
    "            \"attention_mask\": chosen_attention_mask,\n",
    "        },\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        padding_side=\"left\",\n",
    "    )\n",
    "    rejected_inputs = tokenizer.pad(\n",
    "        {\n",
    "            \"input_ids\": rejected_input_ids,\n",
    "            \"attention_mask\": rejected_attention_mask,\n",
    "        },\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        padding_side=\"left\",\n",
    "    )\n",
    "    answer_mask_chosen = torch.zeros_like(chosen_inputs[\"input_ids\"])\n",
    "    answer_mask_rejected = torch.zeros_like(rejected_inputs[\"input_ids\"])\n",
    "    for i, s in enumerate(batch):\n",
    "        answer_mask_chosen[\n",
    "            i, answer_mask_chosen.shape[-1] - 1 - s[\"chosen_answer_len\"] :\n",
    "        ] = 1\n",
    "        answer_mask_rejected[\n",
    "            i, answer_mask_chosen.shape[1] - 1 - s[\"rejected_answer_len\"] :\n",
    "        ] = 1\n",
    "    return {\n",
    "        \"chosen_inputs\": chosen_inputs,\n",
    "        \"rejected_inputs\": rejected_inputs,\n",
    "        \"answer_mask_chosen\": answer_mask_chosen,\n",
    "        \"answer_mask_rejected\": answer_mask_rejected,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8497b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The capital of France is\"\n",
    "tokenizer(text, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b7e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].filter(lambda x: len(x[\"instruction\"]) < 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for sample in tqdm(train_dataset.select(range(1000))):\n",
    "    processed_dataset.extend(sample_trans(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37215144",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = Dataset.from_list(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = new_dataset.map(\n",
    "    lambda sample: process_function(sample, tokenizer),\n",
    "    batched=False,\n",
    "    remove_columns=[\"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    new_dataset, batch_size=8, collate_fn=lambda x: collate_fn(x, tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seq_log_prob(model, input_ids, attention_mask, answer_mask):\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    log_probs = log_probs[:, :-1, :]\n",
    "    target_ids = input_ids[:, 1:]\n",
    "    target_log_probs = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    answer_mask = answer_mask[:, 1:]\n",
    "    target_log_probs = target_log_probs * answer_mask\n",
    "    return target_log_probs.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(policy_chosen, policy_rejected, ref_chosen, ref_rejected, beta=0.1):\n",
    "    pi_ratio = policy_chosen - policy_rejected\n",
    "    ref_ratio = ref_chosen - ref_rejected\n",
    "    loss = torch.log(torch.sigmoid(beta * (pi_ratio - ref_ratio))).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbdf029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_llm_dpo(policy_model, ref_model, train_data, optimizer, epochs=3):\n",
    "    data_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=len(dataloader) * epochs,\n",
    "    )\n",
    "    policy_model.train()\n",
    "    ref_model.eval()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in tqdm(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            chosen_ids = batch[\"chosen_inputs\"][\"input_ids\"].to(device)\n",
    "            rejected_ids = batch[\"rejected_inputs\"][\"input_ids\"].to(device)\n",
    "            attention_mask_chosen = batch[\"chosen_inputs\"][\"attention_mask\"].to(device)\n",
    "            attention_mask_rejected = batch[\"rejected_inputs\"][\"attention_mask\"].to(\n",
    "                device\n",
    "            )\n",
    "            answer_mask_chosen = batch[\"answer_mask_chosen\"].to(device)\n",
    "            answer_mask_rejected = batch[\"answer_mask_rejected\"].to(device)\n",
    "            policy_chosen_log_probs = calculate_seq_log_prob(\n",
    "                policy_model, chosen_ids, attention_mask_chosen, answer_mask_chosen\n",
    "            )\n",
    "            policy_rejected_log_probs = calculate_seq_log_prob(\n",
    "                policy_model,\n",
    "                rejected_ids,\n",
    "                attention_mask_rejected,\n",
    "                answer_mask_rejected,\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                ref_chosen_log_probs = calculate_seq_log_prob(\n",
    "                    ref_model, chosen_ids, attention_mask_chosen, answer_mask_chosen\n",
    "                )\n",
    "                ref_rejected_log_probs = calculate_seq_log_prob(\n",
    "                    ref_model,\n",
    "                    rejected_ids,\n",
    "                    attention_mask_rejected,\n",
    "                    answer_mask_rejected,\n",
    "                )\n",
    "            loss = dpo_loss(\n",
    "                policy_chosen=policy_chosen_log_probs,\n",
    "                policy_rejected=policy_rejected_log_probs,\n",
    "                ref_chosen=ref_chosen_log_probs,\n",
    "                ref_rejected=ref_rejected_log_probs,\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del policy_chosen_log_probs, policy_rejected_log_probs\n",
    "            del ref_chosen_log_probs, ref_rejected_log_probs\n",
    "            torch.cuda.empty_cache()\n",
    "            lr_scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(data_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e31ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14e1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model_policy.parameters(), lr=1e-5)\n",
    "epochs = 3\n",
    "train_llm_dpo(model_policy, model_ref, new_dataset, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64264ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
