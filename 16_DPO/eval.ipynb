{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29992a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lm_eval.models.transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlm_eval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluator\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlm_eval\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM  \u001b[38;5;66;03m# 注意这里的导入路径变化\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lm_eval.models.transformers'"
     ]
    }
   ],
   "source": [
    "from lm_eval import evaluator\n",
    "from lm_eval.models.transformers import AutoModelForCausalLM  # 注意这里的导入路径变化\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def evaluate_peft_model(base_model_name, peft_model_path, tasks=[\"wikitext\", \"hellaswag\"]):\n",
    "    # 加载基础模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    \n",
    "    # 合并PEFT适配器\n",
    "    peft_model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "    merged_model = peft_model.merge_and_unload()  # 关键：合并权重\n",
    "    \n",
    "    # 包装为lm-evaluation-harness兼容的模型\n",
    "    lm = AutoModelForCausalLM(\n",
    "        pretrained=merged_model,  # 使用合并后的模型\n",
    "        tokenizer=tokenizer,\n",
    "        device=\"cuda\" if next(merged_model.parameters()).is_cuda else \"cpu\",\n",
    "        batch_size=2  # 根据显存调整\n",
    "    )\n",
    "    \n",
    "    # 运行评估\n",
    "    results = evaluator.simple_evaluate(\n",
    "        model=lm,\n",
    "        tasks=tasks,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # 打印结果表格\n",
    "    print(evaluator.make_table(results))\n",
    "    return results\n",
    "\n",
    "# 示例：评估PEFT微调后的模型\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_peft_model(\n",
    "        base_model_name=\"../model_save/base_model/qwen-1.5-1.8b/\",\n",
    "        peft_model_path=\"../model_save/dpo_model/qwen-1.5-1.8b-dpo\",  # PEFT适配器路径\n",
    "        tasks=[\"mmlu\"]  # 评估任务\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13e050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
