{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416af3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复现Transformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca9b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055c2ed0",
   "metadata": {},
   "source": [
    "**TransformerEmbedding**\n",
    "\n",
    "词嵌入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.emb(input_ids) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce318da5",
   "metadata": {},
   "source": [
    "**Positional Embedding**\n",
    "\n",
    "Transformer中使用的位置编码为绝对位置编码，使用Sinusoidal函数生成：\n",
    "\n",
    "$$\n",
    "p_{i,2t} = \\sin (i/10000^{2t/d})\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_{i,2t+1} = \\cos (i/10000^{2t/d})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0f11bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model=768, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # 创建位置编码矩阵\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).type(torch.float32)\n",
    "        # 这里使用对数指数的形式计算分数，数值更稳定，硬件上对对数和指数的计算有优化\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2)) * (\n",
    "            -math.log(10000.0) / d_model\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", self.pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        # pe: [1, max_len, d_model]\n",
    "        return x + self.pe[:, : x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a596f",
   "metadata": {},
   "source": [
    "**MultiHeadAttention**\n",
    "\n",
    "多头注意力机制，计算公式如下：\n",
    "$$O_i = \\text{softmax}{\\frac{Q_iK_i^T}{\\sqrt{d}}} V_i$$\n",
    "其中 $Q_i$，$K_i$ 代表注意力头拆分后的第 $i$ 个向量，$d$ 为拆分后的特征向量的维度。\n",
    "在具体实现过程中，注意是对softmax后的注意力进行dropout，如果序列中存在padding，需要对注意力进行遮盖，具体做法是在softmax前需要遮盖的注意力赋予一个非常小的数（负数），这样softmax的时候就会将其权重计算接近0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09accccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        dropout_prob,\n",
    "    ):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        # 进行投影\n",
    "        q = self.q_proj(query)  # [batch, seq_len, emb]\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "        # 进行多头拆分\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        b, s_q, e = q.shape\n",
    "        _, s_k, _ = k.shape\n",
    "        h = self.num_heads\n",
    "        q = q.reshape(b, s_q, h, e // h).transpose(1, 2)  # [b,h,s,e//h]\n",
    "        k = k.reshape(b, s_k, h, e // h).transpose(1, 2)\n",
    "        v = v.reshape(b, s_k, h, e // h).transpose(1, 2)\n",
    "        # 计算qk注意力\n",
    "        att = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.d_model)\n",
    "        if mask is not None:\n",
    "            # mask: [b,s_q,s_k]\n",
    "            # att: [b,h,s_q,s_k]\n",
    "            att.masked_fill_(mask == 0, float(\"-inf\"))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        out = torch.matmul(att, v)  # [b, h, s_q, e//h]\n",
    "        out = out.transpose(1, 2).reshape(b, s_q, e)\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d9a79",
   "metadata": {},
   "source": [
    "**PositionWiseFeedForward**\n",
    "\n",
    "位置感知前馈神经网络，对每个位置的向量单独的进行线性变换，共享权重参数，而不是将序列的所有特征拼接起来进行前馈传播。有两个线性层构成，第一个线性层通常会进行一个升维，然后经过dropout和激活函数，再通过第二个激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, in_feature_dim=768, hidden_size=1024, dropout_prob=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_feature_dim, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, in_feature_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(torch.relu(self.dropout(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee7d8a8",
   "metadata": {},
   "source": [
    "**LayerNorm**\n",
    "\n",
    "层归一化，在特征维度上对特征进行缩放和偏移。公式如下：\n",
    "\n",
    "$$\\text{LayerNorm} = \\gamma \\odot \\frac{X-\\mu}{\\sigma+\\epsilon} + \\beta$$\n",
    "\n",
    "其中 $\\mu$ 是特征均值，$\\sigma$ 是标准差：\n",
    "$$\\mu = \\frac{1}{d}\\sum_{i=1}^dx_i$$\n",
    "\n",
    "$$\\sigma=\\sqrt{\\frac{1}{d}\\sum_{i=1}^d(x_i-\\mu)}$$\n",
    "$d$ 是特征维度。\n",
    "\n",
    "$\\gamma$ 是可学习的缩放参数，$\\beta$ 是可学习的偏移参数，$\\epsilon$ 确保不会除0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893bd2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_dim):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(num_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_dim))\n",
    "        self.epsilon = 1e-6\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)  # [b,s,1]\n",
    "        std = torch.std(x, dim=-1, keepdim=True)\n",
    "        x = self.gamma * (x - mean) / (std + self.epsilon) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bea52",
   "metadata": {},
   "source": [
    "**ResidualConnection**\n",
    "\n",
    "残差连接，也就是结构图中的`Add&Norm`，在实现上，包括两种方式，一种PreNorm，再对子层的结果进行LayerNorm之后再与之前的结果加和；另一种PostNorm，将子层结果和之前结果加和之后再通过LayerNorm。PreNorm训练梯度更稳定，适用于深层的网络，PostNorm训练稳定性较差，但是性能更好。Transformer中采用的是PostNorm，在更大规模的模型中通常使用的是PreNorm。\n",
    "\n",
    "PreNorm：\n",
    "\n",
    "$$\n",
    "\\bm{X} = \\bm{X} + \\text{LayerNorm}(\\text{SubLayer}(\\bm{X}))\n",
    "$$\n",
    "\n",
    "PostNorm：\n",
    "\n",
    "$$\n",
    "\\bm{X} =  \\text{LayerNorm}(\\bm{X} +\\text{SubLayer}(\\bm{X}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, num_dim, mode: Literal[\"PreNorm\", \"PostNorm\"] = \"PostNorm\"):\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.layernorm = LayerNorm(num_dim=num_dim)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        if self.mode == \"PreNorm\":\n",
    "            return x + self.layernorm(sublayer(x))\n",
    "        return self.layernorm(x + sublayer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964d46d4",
   "metadata": {},
   "source": [
    "**EncoderLayer**\n",
    "\n",
    "Encoder的基础模块，结构如下图所示：\n",
    "<div align=\"center\">\n",
    "<img src=\"img/TransformerEncoderLayer.png\" width=250px align=\"center\">\n",
    "</div>\n",
    "\n",
    "主要包含一个多头注意力模块和一个前馈神经网络模块，两个模块分别有两个残差链接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model=768,\n",
    "        hidden_size=1024,\n",
    "        num_att_heads=6,\n",
    "        dropout_prob=0.1,\n",
    "        norm_mode: Literal[\"PreNorm\", \"PostNorm\"] = None,\n",
    "    ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_att_heads,\n",
    "            dropout_prob=dropout_prob,\n",
    "        )\n",
    "        self.res_connect1 = ResidualConnection(\n",
    "            num_dim=d_model,\n",
    "            mode=norm_mode,\n",
    "        )\n",
    "        self.ffn = PositionWiseFeedForward(\n",
    "            in_feature_dim=d_model,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_prob=dropout_prob,\n",
    "        )\n",
    "        self.res_connect2 = ResidualConnection(\n",
    "            num_dim=d_model,\n",
    "            mode=norm_mode,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.res_connect1(x, lambda x: self.multi_head_attention(x, x, x, mask))\n",
    "        x = self.res_connect2(x, lambda x: self.ffn(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bf3f16",
   "metadata": {},
   "source": [
    "**DecoderLayer**\n",
    "\n",
    "Decoder的基础模块，结构图如下所示：\n",
    "<div align=\"center\">\n",
    "<img src=\"img/TransformerDecoderLayer.png\" width=250px>\n",
    "</div>\n",
    "\n",
    "主要包含一个target部分的掩码自注意力机制和source（Encoder输出）部分的交叉注意力部分，以及最后的前馈神经网络部分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5957508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        hidden_size,\n",
    "        num_att_heads,\n",
    "        dropout_prob,\n",
    "        norm_mode: Literal[\"PreNorm\", \"PostNorm\"] = None,\n",
    "    ):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_att_heads,\n",
    "            dropout_prob=dropout_prob,\n",
    "        )\n",
    "        self.res_connect1 = ResidualConnection(\n",
    "            num_dim=d_model,\n",
    "            mode=norm_mode,\n",
    "        )\n",
    "        self.cross_attention = MultiHeadAttention(\n",
    "            d_model=d_model,\n",
    "            num_heads=num_att_heads,\n",
    "            dropout_prob=dropout_prob,\n",
    "        )\n",
    "        self.res_connect2 = ResidualConnection(\n",
    "            num_dim=d_model,\n",
    "            mode=norm_mode,\n",
    "        )\n",
    "        self.ffn = PositionWiseFeedForward(\n",
    "            in_feature_dim=d_model,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_prob=dropout_prob,\n",
    "        )\n",
    "        self.res_connect3 = ResidualConnection(\n",
    "            num_dim=d_model,\n",
    "            mode=norm_mode,\n",
    "        )\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # 首先计算tgt的自注意力，注意这里的有一个因果掩码 tgt_mask是一个下三角矩阵\n",
    "        x = self.res_connect1(\n",
    "            tgt, lambda x: self.self_attention(tgt, tgt, tgt, tgt_mask)\n",
    "        )\n",
    "        # 计算交叉注意力机制\n",
    "        x = self.res_connect2(x, lambda x: self.cross_attention(x, src, src, src_mask))\n",
    "        x = self.res_connect3(x, lambda x: self.ffn(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b9426",
   "metadata": {},
   "source": [
    "**Encoder**\n",
    "\n",
    "由多个EncoderLayer构成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd8baaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        hidden_size,\n",
    "        num_att_heads,\n",
    "        dropout_prob,\n",
    "        norm_mode: Literal[\"PreNorm\", \"PostNorm\"],\n",
    "    ):\n",
    "        self.layers = nn.ModuleList(\n",
    "            EncoderLayer(\n",
    "                d_model=d_model,\n",
    "                hidden_size=hidden_size,\n",
    "                num_att_heads=num_att_heads,\n",
    "                dropout_prob=dropout_prob,\n",
    "                norm_mode=norm_mode,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x, sen_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, x, x, sen_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c012c",
   "metadata": {},
   "source": [
    "**Decoder**\n",
    "\n",
    "由多个DecoderLayer堆叠而成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        hidden_size,\n",
    "        num_att_heads,\n",
    "        dropout_prob,\n",
    "        norm_mode: Literal[\"PreNorm\", \"PostNorm\"],\n",
    "    ):\n",
    "        self.layers = nn.ModuleList(\n",
    "            DecoderLayer(\n",
    "                d_model=d_model,\n",
    "                hidden_size=hidden_size,\n",
    "                num_att_heads=num_att_heads,\n",
    "                dropout_prob=dropout_prob,\n",
    "                norm_mode=norm_mode,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(tgt, src, src, src_mask, tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8010353",
   "metadata": {},
   "source": [
    "**Predictor**\n",
    "\n",
    "最后用来预测输出的模块，由一个线性层构成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36c58630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(Predictor).__init__()\n",
    "        self.cls = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.cls(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca051b9",
   "metadata": {},
   "source": [
    "**Transformer**\n",
    "\n",
    "最后组装Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        vocab_size,\n",
    "        hidden_size,\n",
    "        num_attention_heads,\n",
    "        num_encoder_layer,\n",
    "        num_decoder_layer,\n",
    "        dropout_prob,\n",
    "        src_mask_idx,\n",
    "        tgt_mask_idx,\n",
    "        max_len,\n",
    "        norm_mode,\n",
    "    ):\n",
    "        super(Transformer).__init__()\n",
    "        # pos embedding\n",
    "        self.pos_embedding = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "        # Encoder Embedding模块\n",
    "        self.encoder_embedding = TransformerEmbedding(\n",
    "            d_model=d_model, vocab_size=vocab_size\n",
    "        )\n",
    "        # Encoder\n",
    "        self.Encoder = Encoder(\n",
    "            num_layers=num_encoder_layer,\n",
    "            d_model=d_model,\n",
    "            hidden_size=hidden_size,\n",
    "            num_att_heads=num_attention_heads,\n",
    "            dropout_prob=dropout_prob,\n",
    "            norm_mode=norm_mode,\n",
    "        )\n",
    "        #\n",
    "\n",
    "        # Decoder Embedding模块\n",
    "        self.decoder_embedding = TransformerEmbedding(\n",
    "            d_model=d_model, vocab_size=vocab_size\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            num_layers=num_decoder_layer,\n",
    "            d_model=d_model,\n",
    "            hidden_size=hidden_size,\n",
    "            num_att_heads=num_attention_heads,\n",
    "            dropout_prob=dropout_prob,\n",
    "            norm_mode=norm_mode,\n",
    "        )\n",
    "        # Predictor\n",
    "        self.predictor = Predictor(d_model=d_model, vocab_size=vocab_size)\n",
    "        self.src_mask_idx = src_mask_idx\n",
    "        self.tgt_mask_idx = tgt_mask_idx\n",
    "\n",
    "    def make_padding_mask(self, q_input_ids, k_input_ids, q_pad_idx, k_pad_idx):\n",
    "        # input_ids: [b,s]\n",
    "        # pad_idx: LongTensor\n",
    "        q_mask = q_input_ids.ne(q_pad_idx) #[b,l]\n",
    "        k_mask = k_input_ids.ne(k_pad_idx) #[b,s]\n",
    "        mask = q_mask.unsqueeze(-1) & k_mask()\n",
    "\n",
    "    def forward(self, src_input_ids, tgt_input_ids, src_mask):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
